{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIW+QoPfYpFkOE5K+yr0OO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hy30n80/kaggle/blob/main/Term_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **1. 실험 내용에 대한 전반적 요약**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "때는 2912년, 사람이 살 수 있는 우주의 다른 행성으로 사람들을 수송하던 우주선 타이타닉이 시 공간 이상과 충돌하는 일이 발생하였는데, 승객 데이터를 기반으로 어떤 승객이 충돌로 인해 다른 차원으로 이동되었는지 여부를 예측하는 모델을 도출하는 것이 목적입니다. "
      ],
      "metadata": {
        "id": "t4Ct9rPU4-E3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **2. 데이터 출처 및 선정한 데이터에 대한 설명**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "본 데이터는 Kaggle 에서 가져왔으며, 가장 먼저 Kaggle 과 Colab 과 연동시키기 위해서 계정에 json 파일을 kaggle 폴더에 업로드 하는 과정을 거쳤습니다.  \n",
        "본 Data 는 target feature 을 포함한 14개의 feature로 구성되어 있으며, Training set 에는 8693 개의 Instance 가 있습니다. \n",
        "\n",
        "  1. **PassengerId** : 각 승객의 고유 ID ( 각 ID는 ggg_pp 형식을 취하며, 여기서 ggg는 승객이 함께 여행하는 그룹을 나타내며 pp는 그룹 내의 번호 )\n",
        "  2. **HopePlane** : 승객이 출발한 행성 \n",
        "  3. **CyroSleep** : 승객이 항해 동안, 동면 하는 것을 선택했는지 여부\n",
        "  4. **Cabin** : 승객이 머물고 있는 객실 번호 ( 폼 Deck/Num/Side 을 사용하며, 여기서 측면은 좌현의 경우 P 또는 우현의 경우 S가 될 수 있다 )\n",
        "  5. **Destination** : 승객의 목적지 행성\n",
        "  6. **Age** : 승객의 나이\n",
        "  7. **VIP** : 승객이 항해 중 특별 VIP 서비스를 신청했는지 여부\n",
        "  \n",
        "  8. **RoomService / FoodCourt /  ShoppingMall / Spa / VRDeck** : 승객이 타이타닉의 각 고급 편의시설에 결제한 금액\n",
        "  13. **Name** : 승객의 이름과 성\n",
        "  14. **Transported ( = target)** : 승객이 다른 차원으로 이동되었는지 여부 ( = 예측하려는 대상 ) \n",
        "\n"
      ],
      "metadata": {
        "id": "HlGVkB9v6Lvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#kaggle 계정 토큰 json file 업로드\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "pv8DBUpqo_v2",
        "outputId": "b78e11e3-e4d0-45e7-e60b-72aa586dc424"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36665973-2986-4d96-83c4-df22cec90a1c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-36665973-2986-4d96-83c4-df22cec90a1c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"hy30n80\",\"key\":\"fd641521b37ab39e25b1af679f36e7a4\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# json 파일을 ~/.kaggle 로 이동\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Permission Warning 방지\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "uZb_2ikMp5Bf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#캐글에 있는 타이타닉 승객 데이터 다운로드\n",
        "!kaggle competitions download -c spaceship-titanic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbKy5Uykp9Xx",
        "outputId": "ef6472a6-9d6d-4d1a-818a-32f58a1123b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading spaceship-titanic.zip to /content\n",
            "100% 299k/299k [00:00<00:00, 493kB/s]\n",
            "100% 299k/299k [00:00<00:00, 493kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 압축 풀기\n",
        "!unzip -qq \"/content/spaceship-titanic.zip\""
      ],
      "metadata": {
        "id": "7FqqKFM7qTCS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test 데이터 셋은 label 이 비어있기 때문에, 이번 과제에서는 df_train 으로만 train, validation, test set 을 구성하였습니다. \n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"/content/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/test.csv\")"
      ],
      "metadata": {
        "id": "vX6_aXdjskrZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**캐글에서 불러온 데이터는 df_train 과 df_test 으로 분리되어 있는데, 캐글에서 자체적으로 모델을 평가하기 위한 df_test 은 label 이 비어있기 때문에, df_train 으로만  Train, Validation, Test set 을 따로 구축하기로 하였다. 즉, 이번 과제에서는 캐글 데이터의 df_test 는 함께 전처리만 할 뿐, 학습 및 평가 과정에서 사용하지 않는다.**"
      ],
      "metadata": {
        "id": "JiaKU3UDw1fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "> **3. 데이터 확인 및 전처리**\n",
        "\n",
        "---\n",
        "1. 데이터 확인\n",
        "\n",
        "- 14개의 Feature 중에, numeric data는 6개 {Age , Roomservice, FoodCourt, ShoppingMall, Spa, VRDeck} \n",
        "\n",
        "\n",
        "- Categorical data 는 8개 {PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP, Name, Transported}\n",
        "\n",
        "---\n",
        "\n",
        "2. Feature selection \n",
        "\n",
        "- Feature 'Cabin' 하나에 데크, 번호, 좌우현 총 세 가지 속성을 갖고 있기 때문에, 이들을 각기 독립적인 세 가지의 feature 로 분리하였다. \n",
        "- Output 과 연관이 없는 feature 인 'PassengerId', 'Name' 삭제하였다.\n",
        "\n",
        "---\n",
        "\n",
        "3. 결측값 해결\n",
        "- Categorical feature 의 결측값은 해당 feature 에서 가장 높은 빈도를 갖는 값으로 대체하였다.\n",
        "- Numerical feature 의 결측값은 해당 feature 의 중간값으로 대체하였다.\n",
        "\n",
        "---\n",
        "4. 데이터 가공\n",
        "- Numerical feature 의 값들은 각 Feature 의 평균과 표준편차를 각각 고려하여 정규화하였다. (평균 =0 , 분산 = 1 으로 Standardization )\n",
        "- Categorical feature 의 값들은 원 핫 인코딩을 진행하여 featue 를 확장하였다. \n",
        "\n",
        "---\n",
        "5. 데이터 분할\n",
        "- Train : Validation : Test = 6 : 2 : 2 로 분할하였다. "
      ],
      "metadata": {
        "id": "zN4BrZsnsqGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eGYkpuEGsh_7",
        "outputId": "1264110b-b274-4ae9-8a29-ddef832046c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
              "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
              "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
              "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
              "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
              "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
              "\n",
              "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
              "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
              "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
              "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
              "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
              "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
              "\n",
              "   Transported  \n",
              "0        False  \n",
              "1         True  \n",
              "2        False  \n",
              "3        False  \n",
              "4         True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6cecda7-8e3b-46c0-a74b-cf659f55a76b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>B/0/P</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>39.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Maham Ofracculy</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>24.0</td>\n",
              "      <td>False</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>Juanna Vines</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0003_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>58.0</td>\n",
              "      <td>True</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Altark Susent</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003_02</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>33.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>Solam Susent</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/1/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>16.0</td>\n",
              "      <td>False</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Willy Santantines</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6cecda7-8e3b-46c0-a74b-cf659f55a76b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6cecda7-8e3b-46c0-a74b-cf659f55a76b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6cecda7-8e3b-46c0-a74b-cf659f55a76b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train, test data 합쳐서 전처리 진행.\n",
        "data = pd.concat([df_train,df_test], axis=0, ignore_index=True)\n",
        "\n",
        "#Cabin 안에 3가지 정보가 모여있기 때문에, 3개로 분리하는 과정\n",
        "data = pd.concat([data, data['Cabin'].str.split('/',expand = True)], axis=1)\n",
        "data.rename(columns={0: 'Deck', 1:'Num', 2:'Side'}, inplace=True)\n",
        "\n",
        "#관련 없는 'PassengerId' , 'Name' 삭제, 중복되는 'Cabin' 삭제, Cabin 의 'Num' 은 너무 많은 수를 encoding 해야 하기 때문에 그냥 삭제\n",
        "#결과적으로는 Cabin 의 Deck, Side 정보만 쓰는 것.\n",
        "data.drop(columns = ['PassengerId', 'Name', 'Cabin', 'Num'],inplace =True)\n",
        "\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCgl_bDSOvKo",
        "outputId": "a83e6a5b-aae4-4031-cd28-f3e803b93d20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HomePlanet       288\n",
              "CryoSleep        310\n",
              "Destination      274\n",
              "Age              270\n",
              "VIP              296\n",
              "RoomService      263\n",
              "FoodCourt        289\n",
              "ShoppingMall     306\n",
              "Spa              284\n",
              "VRDeck           268\n",
              "Transported     4277\n",
              "Deck             299\n",
              "Side             299\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#각 categorical data 상세 정보\n",
        "for col in data.columns:\n",
        "  if data[col].dtype==object:\n",
        "    print(col,'\\b:')\n",
        "    print(data[col].value_counts(dropna=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5i61qWqQx-_",
        "outputId": "ad6b6212-df96-412b-f792-3f0028c7adfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HomePlanet \b:\n",
            "Earth     6865\n",
            "Europa    3133\n",
            "Mars      2684\n",
            "NaN        288\n",
            "Name: HomePlanet, dtype: int64\n",
            "CryoSleep \b:\n",
            "False    8079\n",
            "True     4581\n",
            "NaN       310\n",
            "Name: CryoSleep, dtype: int64\n",
            "Destination \b:\n",
            "TRAPPIST-1e      8871\n",
            "55 Cancri e      2641\n",
            "PSO J318.5-22    1184\n",
            "NaN               274\n",
            "Name: Destination, dtype: int64\n",
            "VIP \b:\n",
            "False    12401\n",
            "NaN        296\n",
            "True       273\n",
            "Name: VIP, dtype: int64\n",
            "Transported \b:\n",
            "True     4378\n",
            "False    4315\n",
            "NaN      4277\n",
            "Name: Transported, dtype: int64\n",
            "Deck \b:\n",
            "F      4239\n",
            "G      3781\n",
            "E      1323\n",
            "B      1141\n",
            "C      1102\n",
            "D       720\n",
            "A       354\n",
            "NaN     299\n",
            "T        11\n",
            "Name: Deck, dtype: int64\n",
            "Side \b:\n",
            "S      6381\n",
            "P      6290\n",
            "NaN     299\n",
            "Name: Side, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical feature 의 결측값은 해당 feature 에서 다수를 차지하고 있는 값들로 Replace\n",
        "data['HomePlanet'].fillna('Earth',inplace=True)\n",
        "data['CryoSleep'].fillna(False,inplace=True)\n",
        "data['Destination'].fillna('TRAPPIST-1e',inplace=True)\n",
        "data['VIP'].fillna(False,inplace=True)\n",
        "data['Deck'].fillna('F',inplace=True)\n",
        "\n",
        "#Side 은 결측값 발생한 instance 전의 instance 값으로\n",
        "data['Side'].fillna(method='ffill',inplace=True)\n",
        "data['Transported'].fillna(method='ffill',inplace=True)\n",
        "\n",
        "#numerical feature 의 결측값은 중간값으로 변경\n",
        "for col in data.columns:\n",
        "    if data[col].dtype!=object:\n",
        "        data[col].fillna(data[col].median(),inplace=True)\n"
      ],
      "metadata": {
        "id": "Zf6KPCg3R2i5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Numerical data 의 평균, 표준편차 확인\n",
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "8Qrzxya3UGF5",
        "outputId": "616857b0-3c32-4a8d-84d6-5bea9eecb9e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Age   RoomService     FoodCourt  ShoppingMall           Spa  \\\n",
              "count  12970.000000  12970.000000  12970.000000  12970.000000  12970.000000   \n",
              "mean      28.735081    218.378026    441.890979    170.779491    301.722282   \n",
              "std       14.238958    641.766201   1568.038076    584.153630   1118.746785   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%       20.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%       27.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "75%       37.000000     42.750000     62.750000     23.000000     50.000000   \n",
              "max       79.000000  14327.000000  29813.000000  23492.000000  22408.000000   \n",
              "\n",
              "             VRDeck  \n",
              "count  12970.000000  \n",
              "mean     300.450270  \n",
              "std     1168.655639  \n",
              "min        0.000000  \n",
              "25%        0.000000  \n",
              "50%        0.000000  \n",
              "75%       36.000000  \n",
              "max    24133.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93a67898-f35a-45ab-b58b-b8d7e5bdb089\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>12970.000000</td>\n",
              "      <td>12970.000000</td>\n",
              "      <td>12970.000000</td>\n",
              "      <td>12970.000000</td>\n",
              "      <td>12970.000000</td>\n",
              "      <td>12970.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>28.735081</td>\n",
              "      <td>218.378026</td>\n",
              "      <td>441.890979</td>\n",
              "      <td>170.779491</td>\n",
              "      <td>301.722282</td>\n",
              "      <td>300.450270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>14.238958</td>\n",
              "      <td>641.766201</td>\n",
              "      <td>1568.038076</td>\n",
              "      <td>584.153630</td>\n",
              "      <td>1118.746785</td>\n",
              "      <td>1168.655639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>27.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.000000</td>\n",
              "      <td>42.750000</td>\n",
              "      <td>62.750000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79.000000</td>\n",
              "      <td>14327.000000</td>\n",
              "      <td>29813.000000</td>\n",
              "      <td>23492.000000</td>\n",
              "      <td>22408.000000</td>\n",
              "      <td>24133.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93a67898-f35a-45ab-b58b-b8d7e5bdb089')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93a67898-f35a-45ab-b58b-b8d7e5bdb089 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93a67898-f35a-45ab-b58b-b8d7e5bdb089');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Z-score 를 통한 Numerical 데이터 정규화 과정 / 각 Feature 의 평균과 표준편차를 각각 고려하여 정규화 (평균 =0 , 분산 = 1 으로 Standardization )\n",
        "data_std = data.copy()\n",
        "for col in data_std:\n",
        "  if data_std[col].dtype=='float64':\n",
        "    data_std[col] = (data_std[col]-data_std[col].mean())/data_std[col].std()"
      ],
      "metadata": {
        "id": "x99ELp52WdSN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0o9zkc7UXCFs",
        "outputId": "f9255afc-8d63-4515-e20e-050f1ba51a28"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      HomePlanet  CryoSleep    Destination       Age    VIP  RoomService  \\\n",
              "0         Europa      False    TRAPPIST-1e  0.720904  False    -0.340277   \n",
              "1          Earth      False    TRAPPIST-1e -0.332544  False    -0.170433   \n",
              "2         Europa      False    TRAPPIST-1e  2.055271   True    -0.273274   \n",
              "3         Europa      False    TRAPPIST-1e  0.299525  False    -0.340277   \n",
              "4          Earth      False    TRAPPIST-1e -0.894383  False     0.131858   \n",
              "...          ...        ...            ...       ...    ...          ...   \n",
              "12965      Earth       True    TRAPPIST-1e  0.369755  False    -0.340277   \n",
              "12966      Earth      False    TRAPPIST-1e  0.931593  False    -0.340277   \n",
              "12967       Mars       True    55 Cancri e -0.121854  False    -0.340277   \n",
              "12968     Europa      False    TRAPPIST-1e -0.121854  False    -0.340277   \n",
              "12969      Earth       True  PSO J318.5-22  1.001823  False    -0.340277   \n",
              "\n",
              "       FoodCourt  ShoppingMall       Spa    VRDeck  Transported Deck Side  \n",
              "0      -0.281811     -0.292354 -0.269697 -0.257091        False    B    P  \n",
              "1      -0.276072     -0.249557  0.221031 -0.219440         True    F    S  \n",
              "2       1.998745     -0.292354  5.732555 -0.215162        False    A    S  \n",
              "3       0.536409      0.342753  2.705954 -0.091943        False    A    S  \n",
              "4      -0.237170     -0.033860  0.235333 -0.255379         True    F    S  \n",
              "...          ...           ...       ...       ...          ...  ...  ...  \n",
              "12965  -0.281811     -0.292354 -0.269697 -0.257091         True    G    S  \n",
              "12966   0.258354     -0.263252 -0.260758 -0.133872         True    F    S  \n",
              "12967  -0.281811     -0.292354 -0.269697 -0.257091         True    D    P  \n",
              "12968   1.427331     -0.292354 -0.269697  0.190432         True    D    P  \n",
              "12969  -0.281811     -0.292354 -0.269697 -0.257091         True    G    S  \n",
              "\n",
              "[12970 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3d5c809-daa0-46cd-ae44-b0cf5249a978\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Transported</th>\n",
              "      <th>Deck</th>\n",
              "      <th>Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>0.720904</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>False</td>\n",
              "      <td>B</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>-0.332544</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.170433</td>\n",
              "      <td>-0.276072</td>\n",
              "      <td>-0.249557</td>\n",
              "      <td>0.221031</td>\n",
              "      <td>-0.219440</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>2.055271</td>\n",
              "      <td>True</td>\n",
              "      <td>-0.273274</td>\n",
              "      <td>1.998745</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>5.732555</td>\n",
              "      <td>-0.215162</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>0.299525</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>0.536409</td>\n",
              "      <td>0.342753</td>\n",
              "      <td>2.705954</td>\n",
              "      <td>-0.091943</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>-0.894383</td>\n",
              "      <td>False</td>\n",
              "      <td>0.131858</td>\n",
              "      <td>-0.237170</td>\n",
              "      <td>-0.033860</td>\n",
              "      <td>0.235333</td>\n",
              "      <td>-0.255379</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12965</th>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>0.369755</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>True</td>\n",
              "      <td>G</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12966</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>0.931593</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>0.258354</td>\n",
              "      <td>-0.263252</td>\n",
              "      <td>-0.260758</td>\n",
              "      <td>-0.133872</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12967</th>\n",
              "      <td>Mars</td>\n",
              "      <td>True</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>True</td>\n",
              "      <td>D</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12968</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>1.427331</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.190432</td>\n",
              "      <td>True</td>\n",
              "      <td>D</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12969</th>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>PSO J318.5-22</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>True</td>\n",
              "      <td>G</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12970 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3d5c809-daa0-46cd-ae44-b0cf5249a978')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3d5c809-daa0-46cd-ae44-b0cf5249a978 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3d5c809-daa0-46cd-ae44-b0cf5249a978');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.get_dummies() 를 활용한 Categorical feature 원 핫 인코딩\n",
        "data_std = pd.get_dummies(data_std)"
      ],
      "metadata": {
        "id": "GQ6lC1ivXKH_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "2ZQ3TdCcX0aD",
        "outputId": "e0f64bf8-968e-49de-a3bb-63c2d4db45e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       CryoSleep       Age    VIP  RoomService  FoodCourt  ShoppingMall  \\\n",
              "0          False  0.720904  False    -0.340277  -0.281811     -0.292354   \n",
              "1          False -0.332544  False    -0.170433  -0.276072     -0.249557   \n",
              "2          False  2.055271   True    -0.273274   1.998745     -0.292354   \n",
              "3          False  0.299525  False    -0.340277   0.536409      0.342753   \n",
              "4          False -0.894383  False     0.131858  -0.237170     -0.033860   \n",
              "...          ...       ...    ...          ...        ...           ...   \n",
              "12965       True  0.369755  False    -0.340277  -0.281811     -0.292354   \n",
              "12966      False  0.931593  False    -0.340277   0.258354     -0.263252   \n",
              "12967       True -0.121854  False    -0.340277  -0.281811     -0.292354   \n",
              "12968      False -0.121854  False    -0.340277   1.427331     -0.292354   \n",
              "12969       True  1.001823  False    -0.340277  -0.281811     -0.292354   \n",
              "\n",
              "            Spa    VRDeck  Transported  HomePlanet_Earth  ...  Deck_A  Deck_B  \\\n",
              "0     -0.269697 -0.257091        False                 0  ...       0       1   \n",
              "1      0.221031 -0.219440         True                 1  ...       0       0   \n",
              "2      5.732555 -0.215162        False                 0  ...       1       0   \n",
              "3      2.705954 -0.091943        False                 0  ...       1       0   \n",
              "4      0.235333 -0.255379         True                 1  ...       0       0   \n",
              "...         ...       ...          ...               ...  ...     ...     ...   \n",
              "12965 -0.269697 -0.257091         True                 1  ...       0       0   \n",
              "12966 -0.260758 -0.133872         True                 1  ...       0       0   \n",
              "12967 -0.269697 -0.257091         True                 0  ...       0       0   \n",
              "12968 -0.269697  0.190432         True                 0  ...       0       0   \n",
              "12969 -0.269697 -0.257091         True                 1  ...       0       0   \n",
              "\n",
              "       Deck_C  Deck_D  Deck_E  Deck_F  Deck_G  Deck_T  Side_P  Side_S  \n",
              "0           0       0       0       0       0       0       1       0  \n",
              "1           0       0       0       1       0       0       0       1  \n",
              "2           0       0       0       0       0       0       0       1  \n",
              "3           0       0       0       0       0       0       0       1  \n",
              "4           0       0       0       1       0       0       0       1  \n",
              "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
              "12965       0       0       0       0       1       0       0       1  \n",
              "12966       0       0       0       1       0       0       0       1  \n",
              "12967       0       1       0       0       0       0       1       0  \n",
              "12968       0       1       0       0       0       0       1       0  \n",
              "12969       0       0       0       0       1       0       0       1  \n",
              "\n",
              "[12970 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa81fa93-1199-4db7-adbf-16dd20809102\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Transported</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>...</th>\n",
              "      <th>Deck_A</th>\n",
              "      <th>Deck_B</th>\n",
              "      <th>Deck_C</th>\n",
              "      <th>Deck_D</th>\n",
              "      <th>Deck_E</th>\n",
              "      <th>Deck_F</th>\n",
              "      <th>Deck_G</th>\n",
              "      <th>Deck_T</th>\n",
              "      <th>Side_P</th>\n",
              "      <th>Side_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>0.720904</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>-0.332544</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.170433</td>\n",
              "      <td>-0.276072</td>\n",
              "      <td>-0.249557</td>\n",
              "      <td>0.221031</td>\n",
              "      <td>-0.219440</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>2.055271</td>\n",
              "      <td>True</td>\n",
              "      <td>-0.273274</td>\n",
              "      <td>1.998745</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>5.732555</td>\n",
              "      <td>-0.215162</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>0.299525</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>0.536409</td>\n",
              "      <td>0.342753</td>\n",
              "      <td>2.705954</td>\n",
              "      <td>-0.091943</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>-0.894383</td>\n",
              "      <td>False</td>\n",
              "      <td>0.131858</td>\n",
              "      <td>-0.237170</td>\n",
              "      <td>-0.033860</td>\n",
              "      <td>0.235333</td>\n",
              "      <td>-0.255379</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12965</th>\n",
              "      <td>True</td>\n",
              "      <td>0.369755</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12966</th>\n",
              "      <td>False</td>\n",
              "      <td>0.931593</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>0.258354</td>\n",
              "      <td>-0.263252</td>\n",
              "      <td>-0.260758</td>\n",
              "      <td>-0.133872</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12967</th>\n",
              "      <td>True</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12968</th>\n",
              "      <td>False</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>1.427331</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.190432</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12969</th>\n",
              "      <td>True</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12970 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa81fa93-1199-4db7-adbf-16dd20809102')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa81fa93-1199-4db7-adbf-16dd20809102 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa81fa93-1199-4db7-adbf-16dd20809102');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_std.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dCkKuXeQVD",
        "outputId": "7a79713b-129c-42ff-e01a-0e6551c71b8f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CryoSleep                       bool\n",
              "Age                          float64\n",
              "VIP                             bool\n",
              "RoomService                  float64\n",
              "FoodCourt                    float64\n",
              "ShoppingMall                 float64\n",
              "Spa                          float64\n",
              "VRDeck                       float64\n",
              "Transported                     bool\n",
              "HomePlanet_Earth               uint8\n",
              "HomePlanet_Europa              uint8\n",
              "HomePlanet_Mars                uint8\n",
              "Destination_55 Cancri e        uint8\n",
              "Destination_PSO J318.5-22      uint8\n",
              "Destination_TRAPPIST-1e        uint8\n",
              "Deck_A                         uint8\n",
              "Deck_B                         uint8\n",
              "Deck_C                         uint8\n",
              "Deck_D                         uint8\n",
              "Deck_E                         uint8\n",
              "Deck_F                         uint8\n",
              "Deck_G                         uint8\n",
              "Deck_T                         uint8\n",
              "Side_P                         uint8\n",
              "Side_S                         uint8\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CryoSleep, VIP, Transported : boolean -> 0 or 1 변환\n",
        "for col in data_std.columns:\n",
        "  if data_std[col].dtype == bool:\n",
        "    data_std[col] = data[col].astype(int)"
      ],
      "metadata": {
        "id": "McXIPS9wfFeH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "j0lcNEtyfuRi",
        "outputId": "ea7bd7a6-04f0-46b2-fc14-2cd3298cbdfc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       CryoSleep       Age  VIP  RoomService  FoodCourt  ShoppingMall  \\\n",
              "0              0  0.720904    0    -0.340277  -0.281811     -0.292354   \n",
              "1              0 -0.332544    0    -0.170433  -0.276072     -0.249557   \n",
              "2              0  2.055271    1    -0.273274   1.998745     -0.292354   \n",
              "3              0  0.299525    0    -0.340277   0.536409      0.342753   \n",
              "4              0 -0.894383    0     0.131858  -0.237170     -0.033860   \n",
              "...          ...       ...  ...          ...        ...           ...   \n",
              "12965          1  0.369755    0    -0.340277  -0.281811     -0.292354   \n",
              "12966          0  0.931593    0    -0.340277   0.258354     -0.263252   \n",
              "12967          1 -0.121854    0    -0.340277  -0.281811     -0.292354   \n",
              "12968          0 -0.121854    0    -0.340277   1.427331     -0.292354   \n",
              "12969          1  1.001823    0    -0.340277  -0.281811     -0.292354   \n",
              "\n",
              "            Spa    VRDeck  Transported  HomePlanet_Earth  ...  Deck_A  Deck_B  \\\n",
              "0     -0.269697 -0.257091            0                 0  ...       0       1   \n",
              "1      0.221031 -0.219440            1                 1  ...       0       0   \n",
              "2      5.732555 -0.215162            0                 0  ...       1       0   \n",
              "3      2.705954 -0.091943            0                 0  ...       1       0   \n",
              "4      0.235333 -0.255379            1                 1  ...       0       0   \n",
              "...         ...       ...          ...               ...  ...     ...     ...   \n",
              "12965 -0.269697 -0.257091            1                 1  ...       0       0   \n",
              "12966 -0.260758 -0.133872            1                 1  ...       0       0   \n",
              "12967 -0.269697 -0.257091            1                 0  ...       0       0   \n",
              "12968 -0.269697  0.190432            1                 0  ...       0       0   \n",
              "12969 -0.269697 -0.257091            1                 1  ...       0       0   \n",
              "\n",
              "       Deck_C  Deck_D  Deck_E  Deck_F  Deck_G  Deck_T  Side_P  Side_S  \n",
              "0           0       0       0       0       0       0       1       0  \n",
              "1           0       0       0       1       0       0       0       1  \n",
              "2           0       0       0       0       0       0       0       1  \n",
              "3           0       0       0       0       0       0       0       1  \n",
              "4           0       0       0       1       0       0       0       1  \n",
              "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
              "12965       0       0       0       0       1       0       0       1  \n",
              "12966       0       0       0       1       0       0       0       1  \n",
              "12967       0       1       0       0       0       0       1       0  \n",
              "12968       0       1       0       0       0       0       1       0  \n",
              "12969       0       0       0       0       1       0       0       1  \n",
              "\n",
              "[12970 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47d31cbc-afc3-4680-a4d1-22d7cbf5da78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Transported</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>...</th>\n",
              "      <th>Deck_A</th>\n",
              "      <th>Deck_B</th>\n",
              "      <th>Deck_C</th>\n",
              "      <th>Deck_D</th>\n",
              "      <th>Deck_E</th>\n",
              "      <th>Deck_F</th>\n",
              "      <th>Deck_G</th>\n",
              "      <th>Deck_T</th>\n",
              "      <th>Side_P</th>\n",
              "      <th>Side_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.720904</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.332544</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.170433</td>\n",
              "      <td>-0.276072</td>\n",
              "      <td>-0.249557</td>\n",
              "      <td>0.221031</td>\n",
              "      <td>-0.219440</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2.055271</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.273274</td>\n",
              "      <td>1.998745</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>5.732555</td>\n",
              "      <td>-0.215162</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.299525</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>0.536409</td>\n",
              "      <td>0.342753</td>\n",
              "      <td>2.705954</td>\n",
              "      <td>-0.091943</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.894383</td>\n",
              "      <td>0</td>\n",
              "      <td>0.131858</td>\n",
              "      <td>-0.237170</td>\n",
              "      <td>-0.033860</td>\n",
              "      <td>0.235333</td>\n",
              "      <td>-0.255379</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12965</th>\n",
              "      <td>1</td>\n",
              "      <td>0.369755</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12966</th>\n",
              "      <td>0</td>\n",
              "      <td>0.931593</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>0.258354</td>\n",
              "      <td>-0.263252</td>\n",
              "      <td>-0.260758</td>\n",
              "      <td>-0.133872</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12967</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12968</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>1.427331</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.190432</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12969</th>\n",
              "      <td>1</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12970 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47d31cbc-afc3-4680-a4d1-22d7cbf5da78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47d31cbc-afc3-4680-a4d1-22d7cbf5da78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47d31cbc-afc3-4680-a4d1-22d7cbf5da78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#합쳐 놓았던 데이터를 train, test 분할\n",
        "train_data = data_std.iloc[:df_train.shape[0],:]\n",
        "test_data = data_std.iloc[df_train.shape[0]:,:]\n",
        "print(train_data.shape, test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T9Ph1UzYRPC",
        "outputId": "e12ec12f-c86d-4a3f-ec8a-1195e3d2edb0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8693, 25) (4277, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#중복된 데이터 처리\n",
        "\"\"\"\n",
        "train_data.drop_duplicates(inplace=True)\n",
        "print(train_data.duplicated().sum())\n",
        "print(train_data.shape, test_data.shape)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "qFpGwQBQFXRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transported -> y 빼주기\n",
        "train_y = train_data['Transported']\n",
        "train_x = train_data.drop(columns='Transported')\n",
        "\n",
        "\"\"\"\n",
        "test_y = test_data['Transported']\n",
        "test_x = test_data.drop(columns='Transported')\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Bs3CMNx0YyOe",
        "outputId": "5672a837-1919-4d87-fbb0-69e3b0329db9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntest_y = test_data['Transported']\\ntest_x = test_data.drop(columns='Transported')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 활용할 데이터를 Train : Validation : Test = 6 : 2 : 2 로 분할\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_med , y_train, y_med = train_test_split(train_x, train_y, train_size = 0.60, shuffle=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_med, y_med, test_size = 0.50, shuffle=True)"
      ],
      "metadata": {
        "id": "rlcvpEm2kWdA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zEufY8Ff3NQ",
        "outputId": "7c7cf3fa-13cd-4cc6-f165-82f6c502c421"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5215, 24) (1739, 24) (1739, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "D8hD7yiW-d1a",
        "outputId": "098c2196-6bb5-4f95-f6ad-5821733159fa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CryoSleep       Age  VIP  RoomService  FoodCourt  ShoppingMall  \\\n",
              "3959          0  0.018605    0    -0.282623  -0.281811     -0.006470   \n",
              "6337          1  1.212513    0    -0.340277  -0.281811     -0.292354   \n",
              "5528          1 -0.121854    0    -0.340277  -0.281811     -0.292354   \n",
              "5205          0 -0.051625    0    -0.340277  -0.281811     -0.292354   \n",
              "3648          0 -0.753923    0    -0.192248  -0.281811     -0.292354   \n",
              "...         ...       ...  ...          ...        ...           ...   \n",
              "3495          1 -0.332544    0    -0.340277  -0.281811     -0.292354   \n",
              "883           0 -0.683693    0    -0.228086  -0.278623     -0.292354   \n",
              "772           0  0.861364    0     1.543587  -0.281811     -0.292354   \n",
              "1185          0 -0.121854    0     1.649545  -0.281811     -0.290642   \n",
              "248           0  2.406420    0    -0.340277  -0.265868     -0.073233   \n",
              "\n",
              "           Spa    VRDeck  HomePlanet_Earth  HomePlanet_Europa  ...  Deck_A  \\\n",
              "3959  0.273769 -0.257091                 1                  0  ...       0   \n",
              "6337 -0.269697 -0.257091                 1                  0  ...       0   \n",
              "5528 -0.269697 -0.257091                 1                  0  ...       0   \n",
              "5205 -0.269697 -0.257091                 0                  0  ...       0   \n",
              "3648 -0.258077  0.275145                 1                  0  ...       0   \n",
              "...        ...       ...               ...                ...  ...     ...   \n",
              "3495 -0.269697 -0.257091                 1                  0  ...       0   \n",
              "883   0.036003 -0.096222                 1                  0  ...       0   \n",
              "772  -0.269697  0.455694                 0                  0  ...       0   \n",
              "1185 -0.260758 -0.249389                 0                  0  ...       0   \n",
              "248  -0.269697  0.803958                 1                  0  ...       0   \n",
              "\n",
              "      Deck_B  Deck_C  Deck_D  Deck_E  Deck_F  Deck_G  Deck_T  Side_P  Side_S  \n",
              "3959       0       0       0       0       0       1       0       1       0  \n",
              "6337       0       0       0       0       0       1       0       0       1  \n",
              "5528       0       0       0       0       0       1       0       0       1  \n",
              "5205       0       0       0       0       1       0       0       1       0  \n",
              "3648       0       0       0       0       1       0       0       1       0  \n",
              "...      ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
              "3495       0       0       0       0       1       0       0       1       0  \n",
              "883        0       0       0       0       1       0       0       1       0  \n",
              "772        0       0       0       0       1       0       0       0       1  \n",
              "1185       0       0       0       1       0       0       0       0       1  \n",
              "248        0       0       0       0       1       0       0       1       0  \n",
              "\n",
              "[5215 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ca94601-315f-4d71-bbba-484bb6d2ab7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>...</th>\n",
              "      <th>Deck_A</th>\n",
              "      <th>Deck_B</th>\n",
              "      <th>Deck_C</th>\n",
              "      <th>Deck_D</th>\n",
              "      <th>Deck_E</th>\n",
              "      <th>Deck_F</th>\n",
              "      <th>Deck_G</th>\n",
              "      <th>Deck_T</th>\n",
              "      <th>Side_P</th>\n",
              "      <th>Side_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3959</th>\n",
              "      <td>0</td>\n",
              "      <td>0.018605</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.282623</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.006470</td>\n",
              "      <td>0.273769</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6337</th>\n",
              "      <td>1</td>\n",
              "      <td>1.212513</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5528</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5205</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.051625</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3648</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.753923</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.192248</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.258077</td>\n",
              "      <td>0.275145</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.332544</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.683693</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.228086</td>\n",
              "      <td>-0.278623</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.096222</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>0</td>\n",
              "      <td>0.861364</td>\n",
              "      <td>0</td>\n",
              "      <td>1.543587</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.455694</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1185</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>0</td>\n",
              "      <td>1.649545</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.290642</td>\n",
              "      <td>-0.260758</td>\n",
              "      <td>-0.249389</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>0</td>\n",
              "      <td>2.406420</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.265868</td>\n",
              "      <td>-0.073233</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.803958</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5215 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ca94601-315f-4d71-bbba-484bb6d2ab7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ca94601-315f-4d71-bbba-484bb6d2ab7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ca94601-315f-4d71-bbba-484bb6d2ab7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **4. 모델 구축**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "      4-0. 참고 BaseLine (0-R, 1-R, NB Classifier)\n",
        "      4-1. Decision Tree \n",
        "      4-2. Perceptron\n",
        "      4-3. MLP \n",
        "      4-4. KNN Classifier\n",
        "      4-5. Ensemble model \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "leQcKYkBqrhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고 BaseLine 1. 0-R\n",
        "print(y_train.value_counts(sort=False))\n",
        "print('Zero-R train accuracy : ', max(y_train.value_counts(sort=False)[0], y_train.value_counts(sort=False)[1]) / y_train.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKRpVew7GUbl",
        "outputId": "4b5fbf9e-d393-4c50-9600-251f5175ddbd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    2618\n",
            "1    2597\n",
            "Name: Transported, dtype: int64\n",
            "Zero-R train accuracy :  0.5020134228187919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature를 고려하지 않고, train 의 label 의 분포 비율을 통한 0-R 정확도가  0.502 가 나왔다. Label 의 0,1 분포가 거의 비슷하다는 점을 시사한다. 앞으로 어떤 모델을 구상하던, 당연히 이 수치보다는 높게 나와야 하는 Baseline 이다.**"
      ],
      "metadata": {
        "id": "GNycarrF3akP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고 BaseLine 2. 1-R\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "total_errors = []\n",
        "\n",
        "xy_train = pd.concat([X_train, y_train], axis=1)\n",
        "for col in xy_train.columns[:-1]:\n",
        "  error = 0\n",
        "  for val in xy_train[col].unique():\n",
        "    length = len(xy_train[xy_train[col] == val])\n",
        "    # print(f\"{col} : {val}, length : {length}\")\n",
        "    error += (length - Counter(xy_train[xy_train[col] == val]['Transported']).most_common()[0][1])\n",
        "  total_errors.append(error)\n",
        "\n",
        "best_feature = xy_train.columns[np.argmin(total_errors)]\n",
        "print(total_errors)\n",
        "print(best_feature)\n",
        "print(\"accuracy of \",best_feature, \" is \",1-np.min(total_errors)/xy_train.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga_HBkzdJwZM",
        "outputId": "ebfac62f-9ba8-47be-e217-e91c0ebfb315"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1481, 2244, 2592, 1611, 1636, 1702, 1587, 1637, 2185, 2199, 2583, 2370, 2575, 2348, 2597, 2379, 2437, 2583, 2431, 2409, 2557, 2597, 2308, 2308]\n",
            "CryoSleep\n",
            "accuracy of  CryoSleep  is  0.7160115052732503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(xy_train[xy_train['CryoSleep'] == 0]['Transported']))\n",
        "print(Counter(xy_train[xy_train['CryoSleep'] == 1]['Transported']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6jKmhsh9Pm2",
        "outputId": "c9d7ea42-51ae-4860-9baa-3c9e9c4b69b1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 2274, 1: 1137})\n",
            "Counter({1: 1460, 0: 344})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_predict = []\n",
        "\n",
        "for c in X_val['CryoSleep']:\n",
        "  if c == 0 : \n",
        "    val_predict.append(0)\n",
        "  else: \n",
        "    val_predict.append(1)\n",
        "\n",
        "val_actual = list(y_val)\n",
        "errors = 0\n",
        "for i in range(len(val_actual)):\n",
        "  if val_predict[i]!=val_actual[i]:\n",
        "    errors +=1\n",
        "      \n",
        "print(1 - errors / len(val_actual))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dkSHk4J-H1R",
        "outputId": "7e039de7-0ac1-4803-e80b-bd536b6763ee"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7170787809085681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train set 을 기반으로, 1-R 을 진행하였을 때 여러 Feature 들 중에서, 'CryoSleep' 을 기준으로 결정을 내릴 때 가장 높은 정확도를 나타낸다는 것을 알 수 있다. 다른 Feature 들을 아무것도 고려하지 않고, 오로지 'CryoSleep' == 0 일 때는 Transported =0 을 반환하고, 'CryoSleep' == 1 일 때에는 Transported = 1을 반환한다고 했을 때 정확도가 0.716 이라는 점이다. 이는 0-R 보다 훨씬 개선된 수치이며, 후에 나올 여러 모델의 최소한의 Baseline 정확도가 될 것 이다. 실제로, CryoSleep 을 기준으로 validation set 에 적용해보았을 때도, 0.717 정확도가 나온다는 것을 알 수 있다.**"
      ],
      "metadata": {
        "id": "PDy-Yz0n871w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고 BaseLine 3, NaiveBayesian Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_NB = GaussianNB()\n",
        "\n",
        "#Gaussian Naive Bayesian Classifier을 fit합니다.\n",
        "model_NB.fit(X_train, y_train)\n",
        "\n",
        "#두 데이터에 대하여 score를 출력합니다.\n",
        "score_NB = model_NB.score(X_val,y_val)\n",
        "predict_NB = model_NB.predict(X_val)\n",
        "print(f'score : {score_NB}')\n",
        "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_val.shape[0], (predict_NB != y_val).sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOISLqcHByNA",
        "outputId": "c82b5796-22d9-4075-e6fe-8bbdb80960b2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score : 0.723404255319149\n",
            "Number of mislabeled points out of a total 1739 points : 481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NaiveBeyesian Classifier 을 사용하였을 때, Validation data set 에 대한 정확도가 0.723 이 나왔다**"
      ],
      "metadata": {
        "id": "cF887jtuBswJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1-1. Decision Tree - 기본\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "print(\"train score\", dt.score(X_train, y_train))\n",
        "print(\"validation score\", dt.score(X_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1di3VuekBsL3",
        "outputId": "1761282c-f842-4a97-bbeb-6657997e4e1f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score 0.9520613614573347\n",
            "test score 0.738355376653249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**어떠한 parameter 를 변경하지 않고, Decision Tree 를 사용하였을 때에 Train score 는 0.952, validation score 는 0.738 이 나왔다. 전반적으로 판단했을 때, training data set에 overfitting 되어 있는 경향이 강하다고 판단했기 때문에, DT 에 관한 parameter tuning 을 진행하기로 하였다.**"
      ],
      "metadata": {
        "id": "5q6QFLdTCEHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1-2. Decision Tree - parameter tuning\n",
        "\n",
        "# max leaf nodes 변경\n",
        "dt1 = DecisionTreeClassifier(max_leaf_nodes = 20)\n",
        "dt1.fit(X_train, y_train)\n",
        "\n",
        "# max_depth 변경 \n",
        "dt2 = DecisionTreeClassifier(max_depth = 5)\n",
        "dt2.fit(X_train, y_train)\n",
        "\n",
        "# min sample split\n",
        "dt3 = DecisionTreeClassifier(min_samples_split = 9)\n",
        "dt3.fit(X_train, y_train)\n",
        "\n",
        "# min sample leaf \n",
        "dt4 = DecisionTreeClassifier(min_samples_leaf = 13)\n",
        "dt4.fit(X_train, y_train)\n",
        "\n",
        "# min_impurity decrease \n",
        "dt5 = DecisionTreeClassifier(min_impurity_decrease = 0.001)\n",
        "dt5.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print('Original DT : ', dt.score(X_val, y_val))\n",
        "print('max_leaf_nodes = 20, DT : ', dt1.score(X_val, y_val))\n",
        "print('max_depth = 5, DT : ', dt2.score(X_val, y_val))\n",
        "print('min_samples_split = 9, DT : ', dt3.score(X_val, y_val))\n",
        "print('min_samples_leaf = 13, DT : ', dt4.score(X_val, y_val))\n",
        "print('min_impurity_decrease = 0.001, DT : ', dt5.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0nNqs5HIi1G",
        "outputId": "a3bf41f6-9392-4ddc-a80f-b52f8d4d5085"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DT :  0.738355376653249\n",
            "max_leaf_nodes = 20, DT :  0.7849338700402531\n",
            "max_depth = 5, DT :  0.7648073605520413\n",
            "min_samples_split = 9, DT :  0.7515813686026452\n",
            "min_samples_leaf = 13, DT :  0.7849338700402531\n",
            "min_impurity_decrease = 0.001, DT :  0.7912593444508338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree 의 여러 pruning 방법을 시도해 보았고, 여러 시행착오를 거치면서 각 parameter 에서 높은 정확도를 나타내는 수치를 발견하였다. 그 중 min_impurity_decrease 를 0.001 으로 했을 때 validation score 가 0.791 로 가장 높은 정확도를 보였고, 그 뒤로는 max_leaf_nodes =20 혹은 min_samples_leaf = 13 으로 train set 에 관한 Decision Tree 를 만들었을 때 validation score 가 0.785 로 초반 DT 에 비하여 괄목할 만한 높은 정확도를 보였다.**"
      ],
      "metadata": {
        "id": "Xa42jBW8DJ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "GDwtFOWY7C5l",
        "outputId": "d992b962-b7c4-4805-bb5c-ce3af5641d6c"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CryoSleep       Age  VIP  RoomService  FoodCourt  ShoppingMall  \\\n",
              "3959          0  0.018605    0    -0.282623  -0.281811     -0.006470   \n",
              "6337          1  1.212513    0    -0.340277  -0.281811     -0.292354   \n",
              "5528          1 -0.121854    0    -0.340277  -0.281811     -0.292354   \n",
              "5205          0 -0.051625    0    -0.340277  -0.281811     -0.292354   \n",
              "3648          0 -0.753923    0    -0.192248  -0.281811     -0.292354   \n",
              "...         ...       ...  ...          ...        ...           ...   \n",
              "3495          1 -0.332544    0    -0.340277  -0.281811     -0.292354   \n",
              "883           0 -0.683693    0    -0.228086  -0.278623     -0.292354   \n",
              "772           0  0.861364    0     1.543587  -0.281811     -0.292354   \n",
              "1185          0 -0.121854    0     1.649545  -0.281811     -0.290642   \n",
              "248           0  2.406420    0    -0.340277  -0.265868     -0.073233   \n",
              "\n",
              "           Spa    VRDeck  HomePlanet_Earth  HomePlanet_Europa  ...  Deck_A  \\\n",
              "3959  0.273769 -0.257091                 1                  0  ...       0   \n",
              "6337 -0.269697 -0.257091                 1                  0  ...       0   \n",
              "5528 -0.269697 -0.257091                 1                  0  ...       0   \n",
              "5205 -0.269697 -0.257091                 0                  0  ...       0   \n",
              "3648 -0.258077  0.275145                 1                  0  ...       0   \n",
              "...        ...       ...               ...                ...  ...     ...   \n",
              "3495 -0.269697 -0.257091                 1                  0  ...       0   \n",
              "883   0.036003 -0.096222                 1                  0  ...       0   \n",
              "772  -0.269697  0.455694                 0                  0  ...       0   \n",
              "1185 -0.260758 -0.249389                 0                  0  ...       0   \n",
              "248  -0.269697  0.803958                 1                  0  ...       0   \n",
              "\n",
              "      Deck_B  Deck_C  Deck_D  Deck_E  Deck_F  Deck_G  Deck_T  Side_P  Side_S  \n",
              "3959       0       0       0       0       0       1       0       1       0  \n",
              "6337       0       0       0       0       0       1       0       0       1  \n",
              "5528       0       0       0       0       0       1       0       0       1  \n",
              "5205       0       0       0       0       1       0       0       1       0  \n",
              "3648       0       0       0       0       1       0       0       1       0  \n",
              "...      ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
              "3495       0       0       0       0       1       0       0       1       0  \n",
              "883        0       0       0       0       1       0       0       1       0  \n",
              "772        0       0       0       0       1       0       0       0       1  \n",
              "1185       0       0       0       1       0       0       0       0       1  \n",
              "248        0       0       0       0       1       0       0       1       0  \n",
              "\n",
              "[5215 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97043564-ea28-4dca-b10a-98b95f286a2e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>...</th>\n",
              "      <th>Deck_A</th>\n",
              "      <th>Deck_B</th>\n",
              "      <th>Deck_C</th>\n",
              "      <th>Deck_D</th>\n",
              "      <th>Deck_E</th>\n",
              "      <th>Deck_F</th>\n",
              "      <th>Deck_G</th>\n",
              "      <th>Deck_T</th>\n",
              "      <th>Side_P</th>\n",
              "      <th>Side_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3959</th>\n",
              "      <td>0</td>\n",
              "      <td>0.018605</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.282623</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.006470</td>\n",
              "      <td>0.273769</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6337</th>\n",
              "      <td>1</td>\n",
              "      <td>1.212513</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5528</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5205</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.051625</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3648</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.753923</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.192248</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.258077</td>\n",
              "      <td>0.275145</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.332544</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>-0.257091</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.683693</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.228086</td>\n",
              "      <td>-0.278623</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.096222</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>0</td>\n",
              "      <td>0.861364</td>\n",
              "      <td>0</td>\n",
              "      <td>1.543587</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.292354</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.455694</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1185</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.121854</td>\n",
              "      <td>0</td>\n",
              "      <td>1.649545</td>\n",
              "      <td>-0.281811</td>\n",
              "      <td>-0.290642</td>\n",
              "      <td>-0.260758</td>\n",
              "      <td>-0.249389</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>0</td>\n",
              "      <td>2.406420</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.340277</td>\n",
              "      <td>-0.265868</td>\n",
              "      <td>-0.073233</td>\n",
              "      <td>-0.269697</td>\n",
              "      <td>0.803958</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5215 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97043564-ea28-4dca-b10a-98b95f286a2e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97043564-ea28-4dca-b10a-98b95f286a2e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97043564-ea28-4dca-b10a-98b95f286a2e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1-3. Decision Tree - Feature selection 적용\n",
        "\n",
        "print(X_train.shape)\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "clf = DecisionTreeClassifier(min_samples_leaf = 13)\n",
        "\n",
        "## 24개의 Feature 중, 가장 중요도 높은 7개의 feature 으로만 활용한 model 의 validation score\n",
        "selector_1 = RFE(clf, n_features_to_select=7, step=1)\n",
        "selector_1 = selector_1.fit(X_train, y_train)\n",
        "\n",
        "clf.fit(X_train.iloc[:,selector_1.support_], y_train)\n",
        "print(clf.score(X_val.iloc[:,selector_1.support_], y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hgvUSFy1dkx",
        "outputId": "98eafd61-5715-4816-c4a8-ebb929915f8f"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5215, 24)\n",
            "0.7722829212190915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24개의 Feature 중, 가장 중요도 높은 10개의 feature 으로만 활용한 model 의 validation score\n",
        "\n",
        "selector_2 = RFE(clf, n_features_to_select=10, step=1)\n",
        "selector_2 = selector_2.fit(X_train, y_train)\n",
        "\n",
        "clf.fit(X_train.iloc[:,selector_2.support_], y_train)\n",
        "print(clf.score(X_val.iloc[:,selector_2.support_], y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKFlItIZ5mkv",
        "outputId": "8a94712a-aec1-4cf7-a1ed-0e42c74333a6"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.765382403680276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24개의 Feature 중, 가장 중요도 높은 13개의 feature 으로만 활용한 model 의 validation score\n",
        "\n",
        "selector_3 = RFE(clf, n_features_to_select=13, step=1)\n",
        "selector_3 = selector_3.fit(X_train, y_train)\n",
        "\n",
        "clf.fit( X_train.iloc[:,selector_3.support_], y_train)\n",
        "print(clf.score(X_val.iloc[:,selector_3.support_], y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QppPqvlZ9NQQ",
        "outputId": "204bbd2c-a86f-44e3-ec9f-3fa422039f04"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7826336975273146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24개의 Feature 중, 가장 중요도 높은 17개의 feature 으로만 활용한 model 의 validation score\n",
        "\n",
        "selector_4 = RFE(clf, n_features_to_select=17, step=1)\n",
        "selector_4 = selector_4.fit(X_train, y_train)\n",
        "\n",
        "clf.fit(X_train.iloc[:,selector_4.support_], y_train)\n",
        "print(clf.score(X_val.iloc[:,selector_4.support_], y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5GLT4kB9rOx",
        "outputId": "ecd30607-9022-4575-b1bf-446abd46ad34"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7866589994249569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24개의 Feature 중, 가장 중요도 높은 21개의 feature 으로만 활용한 model 의 validation score\n",
        "\n",
        "selector_5 = RFE(clf, n_features_to_select=21, step=1)\n",
        "selector_5 = selector_5.fit(X_train, y_train)\n",
        "\n",
        "clf.fit(X_train.iloc[:,selector_5.support_], y_train)\n",
        "print(clf.score(X_val.iloc[:,selector_5.support_], y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt9t1kXk-bsE",
        "outputId": "2e457631-deb5-4c5e-b990-eeab8358b1f8"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7849338700402531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(selector_4.support_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98n8zRR_wzf",
        "outputId": "dab00123-653a-4ac1-90b4-7d8783523b24"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True False  True  True  True  True  True  True  True False False\n",
            "  True  True  True False  True False  True  True  True False  True False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n_features_to_select=10 인 Decision Tree 의 성능 validation score 는 0.7849 였으며, Feature selection 을 통해 더 높일 수 있다고 생각하였다. 실제로 원하는 개수의 변수들이 남을 때까지 학습을 반복하며 유의미하지 않은 변수들을 제거해나가는 Backward 방식인 RFE 방식을 사용하였는데, 24개의 feature 중 17개의 feature 들만 사용하여 모델을 구축하고 평가했을 때 0.7867 으로 작은 폭이지만, 성능이 증가한 모습을 확인할 수 있었다.\n",
        "이에 관한 원인으로, 현재 7개의 삭제된 feature 가 대부분 Deck 에 관련한 것이였는데 아무래도 Deck 에 관한 feature 가 하나의 Feature 에서 원 핫 인코딩을 통해 8가지의 Feautre 로 분할되면서 분류 기준으로서 분별 영향력이 줄어들었기 때문이라고 생각한다.**  "
      ],
      "metadata": {
        "id": "uyAuAsox1i6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2-1. Perceptron - (iter=10) \n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "model_perceptron_1 = Perceptron(max_iter=10)\n",
        "model_perceptron_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"train score\", model_perceptron_1.score(X_train, y_train))\n",
        "print(\"validation score\", model_perceptron_1.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNU0DKvJBqFN",
        "outputId": "d2c5120d-08b6-4a71-86b6-c0b01ac1bc86"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score 0.6632790028763184\n",
            "validation score 0.6503737780333525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2-2. Perceptron - (iter = 100) \n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "model_perceptron_2 = Perceptron(max_iter=100)\n",
        "model_perceptron_2.fit(X_train, y_train)\n",
        "\n",
        "print(\"train score\", model_perceptron_2.score(X_train, y_train))\n",
        "print(\"validation score\", model_perceptron_2.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN3rp1MMHNgr",
        "outputId": "a792d486-5eeb-4cc1-ec10-8a90b7500ed5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score 0.7018216682646213\n",
            "validation score 0.718803910293272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2-3. Perceptron - (iter = 1000)\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "model_perceptron_3 = Perceptron(max_iter=1000)\n",
        "model_perceptron_3.fit(X_train, y_train)\n",
        "\n",
        "print(\"train score\", model_perceptron_3.score(X_train, y_train))\n",
        "print(\"validation score\", model_perceptron_3.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT1jQHP-E64F",
        "outputId": "c70b6a52-9b76-4267-e909-ac6eabe81e22"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score 0.7018216682646213\n",
            "validation score 0.718803910293272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron 을 기반으로 모델을 구축해보았다. max_iter 을 10, 100, 1000 으로 각기 달리하여 3가지의 perceptron 을 만들어보았는데, 처음 max_iter = 10 일 때 validation score 가 약 0.65 정도의 낮은 정확도를 보였는데, train score 도 비슷한 수치의 정확도였기 때문에 모델의 문제가 overfitting 이 아닌, underfitting 이라는 점을 알 수 있었다. 따라서, iteration 을 100, 1000 으로 늘렸지만 더 이상 학습하지 못하고 정확도가 올라가지 않는다는 점을 확인할 수 있었고, 최종적으로 perceptron 의 구조는 0.719 정확도에서 멈춘다는 사실을 알게 되었다. 이는 위에서 진행한 1-R 의 0.717 과 거의 비슷한 정도였다.**"
      ],
      "metadata": {
        "id": "oTUNggBgFLVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch 에 따른 accuracy 를 시각화하는 plot 함수\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(results):\n",
        "  plt.plot(results.history['accuracy'])\n",
        "  plt.plot(results.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jh7_AJitMoo0"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3-1. MLP (Multilayer perceptron)\n",
        "import tensorflow as tf\n",
        "\n",
        "MLP_1 = tf.keras.models.Sequential([\n",
        "\ttf.keras.layers.Dense(16, input_shape=(24,), activation='relu'),\n",
        "\ttf.keras.layers.Dense(8, activation='relu'), \n",
        "\ttf.keras.layers.Dense(1, activation='sigmoid'), \n",
        "])\n",
        "\n",
        "MLP_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4jgiqfKBujN",
        "outputId": "8ea0ac72-4cb1-4a94-b94d-50e1ac2ee182"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 16)                400       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 545\n",
            "Trainable params: 545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_1.compile(tf.keras.optimizers.RMSprop(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "history_MLP_1 = MLP_1.fit(X_train,y_train,epochs=100,batch_size=32,validation_data=(X_val,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9JloYvYVBnJ",
        "outputId": "584b00bd-acff-4fc9-ff25-3ff92f30c110"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 [==============================] - 1s 5ms/step - loss: 0.6429 - accuracy: 0.6439 - val_loss: 0.5411 - val_accuracy: 0.7516\n",
            "Epoch 2/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4957 - accuracy: 0.7613 - val_loss: 0.4493 - val_accuracy: 0.7872\n",
            "Epoch 3/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4484 - accuracy: 0.7845 - val_loss: 0.4281 - val_accuracy: 0.8022\n",
            "Epoch 4/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4345 - accuracy: 0.7904 - val_loss: 0.4200 - val_accuracy: 0.8028\n",
            "Epoch 5/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4277 - accuracy: 0.7918 - val_loss: 0.4164 - val_accuracy: 0.7993\n",
            "Epoch 6/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4230 - accuracy: 0.7952 - val_loss: 0.4153 - val_accuracy: 0.7987\n",
            "Epoch 7/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4189 - accuracy: 0.7988 - val_loss: 0.4131 - val_accuracy: 0.8016\n",
            "Epoch 8/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4166 - accuracy: 0.7969 - val_loss: 0.4128 - val_accuracy: 0.8010\n",
            "Epoch 9/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4151 - accuracy: 0.7944 - val_loss: 0.4123 - val_accuracy: 0.8005\n",
            "Epoch 10/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4137 - accuracy: 0.7975 - val_loss: 0.4115 - val_accuracy: 0.8045\n",
            "Epoch 11/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4118 - accuracy: 0.7983 - val_loss: 0.4098 - val_accuracy: 0.8028\n",
            "Epoch 12/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4100 - accuracy: 0.7994 - val_loss: 0.4107 - val_accuracy: 0.8010\n",
            "Epoch 13/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4079 - accuracy: 0.8021 - val_loss: 0.4104 - val_accuracy: 0.8045\n",
            "Epoch 14/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4074 - accuracy: 0.7983 - val_loss: 0.4094 - val_accuracy: 0.8056\n",
            "Epoch 15/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4062 - accuracy: 0.8012 - val_loss: 0.4073 - val_accuracy: 0.8102\n",
            "Epoch 16/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4047 - accuracy: 0.8038 - val_loss: 0.4076 - val_accuracy: 0.8062\n",
            "Epoch 17/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4039 - accuracy: 0.8010 - val_loss: 0.4084 - val_accuracy: 0.8074\n",
            "Epoch 18/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4023 - accuracy: 0.8019 - val_loss: 0.4069 - val_accuracy: 0.8074\n",
            "Epoch 19/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4018 - accuracy: 0.8029 - val_loss: 0.4070 - val_accuracy: 0.8079\n",
            "Epoch 20/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4008 - accuracy: 0.8071 - val_loss: 0.4058 - val_accuracy: 0.8074\n",
            "Epoch 21/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4000 - accuracy: 0.8008 - val_loss: 0.4080 - val_accuracy: 0.8056\n",
            "Epoch 22/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3988 - accuracy: 0.8025 - val_loss: 0.4073 - val_accuracy: 0.8068\n",
            "Epoch 23/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3985 - accuracy: 0.8046 - val_loss: 0.4061 - val_accuracy: 0.8074\n",
            "Epoch 24/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3977 - accuracy: 0.8067 - val_loss: 0.4059 - val_accuracy: 0.8097\n",
            "Epoch 25/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3970 - accuracy: 0.8010 - val_loss: 0.4054 - val_accuracy: 0.8125\n",
            "Epoch 26/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3961 - accuracy: 0.8058 - val_loss: 0.4063 - val_accuracy: 0.8074\n",
            "Epoch 27/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3960 - accuracy: 0.8065 - val_loss: 0.4047 - val_accuracy: 0.8079\n",
            "Epoch 28/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3950 - accuracy: 0.8065 - val_loss: 0.4060 - val_accuracy: 0.8108\n",
            "Epoch 29/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3944 - accuracy: 0.8056 - val_loss: 0.4059 - val_accuracy: 0.8102\n",
            "Epoch 30/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3941 - accuracy: 0.8082 - val_loss: 0.4054 - val_accuracy: 0.8114\n",
            "Epoch 31/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3930 - accuracy: 0.8075 - val_loss: 0.4077 - val_accuracy: 0.8062\n",
            "Epoch 32/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3925 - accuracy: 0.8056 - val_loss: 0.4091 - val_accuracy: 0.8085\n",
            "Epoch 33/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3926 - accuracy: 0.8052 - val_loss: 0.4070 - val_accuracy: 0.8102\n",
            "Epoch 34/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3923 - accuracy: 0.8065 - val_loss: 0.4065 - val_accuracy: 0.8091\n",
            "Epoch 35/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3917 - accuracy: 0.8042 - val_loss: 0.4062 - val_accuracy: 0.8154\n",
            "Epoch 36/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3913 - accuracy: 0.8082 - val_loss: 0.4069 - val_accuracy: 0.8120\n",
            "Epoch 37/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3907 - accuracy: 0.8067 - val_loss: 0.4060 - val_accuracy: 0.8125\n",
            "Epoch 38/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3905 - accuracy: 0.8075 - val_loss: 0.4059 - val_accuracy: 0.8114\n",
            "Epoch 39/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3888 - accuracy: 0.8109 - val_loss: 0.4059 - val_accuracy: 0.8097\n",
            "Epoch 40/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3894 - accuracy: 0.8071 - val_loss: 0.4064 - val_accuracy: 0.8125\n",
            "Epoch 41/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3885 - accuracy: 0.8065 - val_loss: 0.4069 - val_accuracy: 0.8102\n",
            "Epoch 42/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3885 - accuracy: 0.8075 - val_loss: 0.4065 - val_accuracy: 0.8062\n",
            "Epoch 43/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3879 - accuracy: 0.8090 - val_loss: 0.4085 - val_accuracy: 0.8097\n",
            "Epoch 44/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3875 - accuracy: 0.8063 - val_loss: 0.4060 - val_accuracy: 0.8074\n",
            "Epoch 45/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3871 - accuracy: 0.8111 - val_loss: 0.4058 - val_accuracy: 0.8097\n",
            "Epoch 46/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3868 - accuracy: 0.8088 - val_loss: 0.4067 - val_accuracy: 0.8085\n",
            "Epoch 47/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3866 - accuracy: 0.8104 - val_loss: 0.4051 - val_accuracy: 0.8114\n",
            "Epoch 48/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3863 - accuracy: 0.8115 - val_loss: 0.4077 - val_accuracy: 0.8068\n",
            "Epoch 49/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3850 - accuracy: 0.8136 - val_loss: 0.4063 - val_accuracy: 0.8114\n",
            "Epoch 50/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3851 - accuracy: 0.8100 - val_loss: 0.4069 - val_accuracy: 0.8097\n",
            "Epoch 51/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3846 - accuracy: 0.8107 - val_loss: 0.4059 - val_accuracy: 0.8102\n",
            "Epoch 52/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3848 - accuracy: 0.8127 - val_loss: 0.4070 - val_accuracy: 0.8074\n",
            "Epoch 53/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3836 - accuracy: 0.8096 - val_loss: 0.4071 - val_accuracy: 0.8114\n",
            "Epoch 54/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3839 - accuracy: 0.8138 - val_loss: 0.4075 - val_accuracy: 0.8056\n",
            "Epoch 55/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3841 - accuracy: 0.8115 - val_loss: 0.4076 - val_accuracy: 0.8091\n",
            "Epoch 56/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3833 - accuracy: 0.8111 - val_loss: 0.4053 - val_accuracy: 0.8102\n",
            "Epoch 57/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3822 - accuracy: 0.8161 - val_loss: 0.4077 - val_accuracy: 0.8051\n",
            "Epoch 58/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3830 - accuracy: 0.8155 - val_loss: 0.4078 - val_accuracy: 0.8114\n",
            "Epoch 59/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3828 - accuracy: 0.8109 - val_loss: 0.4079 - val_accuracy: 0.8085\n",
            "Epoch 60/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3816 - accuracy: 0.8161 - val_loss: 0.4062 - val_accuracy: 0.8102\n",
            "Epoch 61/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3816 - accuracy: 0.8121 - val_loss: 0.4084 - val_accuracy: 0.8068\n",
            "Epoch 62/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3820 - accuracy: 0.8155 - val_loss: 0.4057 - val_accuracy: 0.8114\n",
            "Epoch 63/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3815 - accuracy: 0.8151 - val_loss: 0.4087 - val_accuracy: 0.8114\n",
            "Epoch 64/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3814 - accuracy: 0.8148 - val_loss: 0.4065 - val_accuracy: 0.8125\n",
            "Epoch 65/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3809 - accuracy: 0.8142 - val_loss: 0.4079 - val_accuracy: 0.8074\n",
            "Epoch 66/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3809 - accuracy: 0.8163 - val_loss: 0.4069 - val_accuracy: 0.8097\n",
            "Epoch 67/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3809 - accuracy: 0.8136 - val_loss: 0.4079 - val_accuracy: 0.8091\n",
            "Epoch 68/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3799 - accuracy: 0.8176 - val_loss: 0.4090 - val_accuracy: 0.8085\n",
            "Epoch 69/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3800 - accuracy: 0.8134 - val_loss: 0.4085 - val_accuracy: 0.8085\n",
            "Epoch 70/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3803 - accuracy: 0.8165 - val_loss: 0.4068 - val_accuracy: 0.8108\n",
            "Epoch 71/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3796 - accuracy: 0.8127 - val_loss: 0.4061 - val_accuracy: 0.8085\n",
            "Epoch 72/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3797 - accuracy: 0.8169 - val_loss: 0.4099 - val_accuracy: 0.8120\n",
            "Epoch 73/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3797 - accuracy: 0.8161 - val_loss: 0.4074 - val_accuracy: 0.8062\n",
            "Epoch 74/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3797 - accuracy: 0.8130 - val_loss: 0.4083 - val_accuracy: 0.8079\n",
            "Epoch 75/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3790 - accuracy: 0.8159 - val_loss: 0.4075 - val_accuracy: 0.8108\n",
            "Epoch 76/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3790 - accuracy: 0.8146 - val_loss: 0.4080 - val_accuracy: 0.8114\n",
            "Epoch 77/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3795 - accuracy: 0.8142 - val_loss: 0.4072 - val_accuracy: 0.8085\n",
            "Epoch 78/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3792 - accuracy: 0.8148 - val_loss: 0.4077 - val_accuracy: 0.8045\n",
            "Epoch 79/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3784 - accuracy: 0.8163 - val_loss: 0.4078 - val_accuracy: 0.8079\n",
            "Epoch 80/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3784 - accuracy: 0.8148 - val_loss: 0.4083 - val_accuracy: 0.8068\n",
            "Epoch 81/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3776 - accuracy: 0.8174 - val_loss: 0.4074 - val_accuracy: 0.8079\n",
            "Epoch 82/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3777 - accuracy: 0.8142 - val_loss: 0.4069 - val_accuracy: 0.8045\n",
            "Epoch 83/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3771 - accuracy: 0.8178 - val_loss: 0.4101 - val_accuracy: 0.8079\n",
            "Epoch 84/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3779 - accuracy: 0.8157 - val_loss: 0.4075 - val_accuracy: 0.8097\n",
            "Epoch 85/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3771 - accuracy: 0.8178 - val_loss: 0.4062 - val_accuracy: 0.8056\n",
            "Epoch 86/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3781 - accuracy: 0.8146 - val_loss: 0.4087 - val_accuracy: 0.8056\n",
            "Epoch 87/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3774 - accuracy: 0.8161 - val_loss: 0.4088 - val_accuracy: 0.8074\n",
            "Epoch 88/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3772 - accuracy: 0.8174 - val_loss: 0.4076 - val_accuracy: 0.8091\n",
            "Epoch 89/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3773 - accuracy: 0.8169 - val_loss: 0.4070 - val_accuracy: 0.8102\n",
            "Epoch 90/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3770 - accuracy: 0.8159 - val_loss: 0.4081 - val_accuracy: 0.8102\n",
            "Epoch 91/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3767 - accuracy: 0.8165 - val_loss: 0.4071 - val_accuracy: 0.8097\n",
            "Epoch 92/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3769 - accuracy: 0.8161 - val_loss: 0.4072 - val_accuracy: 0.8068\n",
            "Epoch 93/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3761 - accuracy: 0.8173 - val_loss: 0.4071 - val_accuracy: 0.8085\n",
            "Epoch 94/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3763 - accuracy: 0.8167 - val_loss: 0.4075 - val_accuracy: 0.8102\n",
            "Epoch 95/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3757 - accuracy: 0.8180 - val_loss: 0.4080 - val_accuracy: 0.8114\n",
            "Epoch 96/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3762 - accuracy: 0.8123 - val_loss: 0.4069 - val_accuracy: 0.8114\n",
            "Epoch 97/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3755 - accuracy: 0.8153 - val_loss: 0.4081 - val_accuracy: 0.8062\n",
            "Epoch 98/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3759 - accuracy: 0.8146 - val_loss: 0.4088 - val_accuracy: 0.8051\n",
            "Epoch 99/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3758 - accuracy: 0.8169 - val_loss: 0.4085 - val_accuracy: 0.8108\n",
            "Epoch 100/100\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3754 - accuracy: 0.8192 - val_loss: 0.4089 - val_accuracy: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history_MLP_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "u3Maa__yM-c6",
        "outputId": "f1e9ee12-84a0-4ef0-d309-4b4410a6d356"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP_1 모델은 hidden layer 2개로 모델을 쌓았고, Optimizer 는 RMSprop 으로 learning rate 는 0.001 로 구성하였다. 그 결과, validation accuracy 가 0.81 대까지 올라온 모습을 확인할 수 있었다. 물론 높은 수치였지만, training accuracy 가 validation accuracy 와 비슷한 0.81 대의 수치였기 때문에, 만약 training set 에 조금 더 fitting 된다면 validation accuracy 가 더 높아질 수 있을 것이라는 기대가 생겼고, 2번째 MLP_2 모델은 더 깊게, epochs 수를 늘리기로 결정하였다.**"
      ],
      "metadata": {
        "id": "KkeFpFN5JsOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3-2. MLP (Multilayer perceptron)\n",
        "import tensorflow as tf\n",
        "\n",
        "MLP_2 = tf.keras.models.Sequential([\n",
        "\ttf.keras.layers.Dense(64, input_shape=(24,), activation='relu'), \n",
        "  tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "  tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "\ttf.keras.layers.Dense(8, activation='relu'), \n",
        "\ttf.keras.layers.Dense(1, activation='sigmoid'), \n",
        "])\n",
        "\n",
        "MLP_2.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHTZueEzH8S8",
        "outputId": "2ad726b8-f0d6-46a7-dd89-93ca76805794"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_27 (Dense)            (None, 64)                1600      \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,353\n",
            "Trainable params: 4,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_2.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "history_MLP_2 = MLP_2.fit(X_train,y_train,epochs=300,batch_size=32,validation_data=(X_val,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z55m0jD7INsd",
        "outputId": "aaa27691-29f3-4c9f-ab30-34ccce8c1347"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 [==============================] - 2s 5ms/step - loss: 0.5121 - accuracy: 0.7628 - val_loss: 0.4277 - val_accuracy: 0.7959\n",
            "Epoch 2/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.4341 - accuracy: 0.7839 - val_loss: 0.4190 - val_accuracy: 0.7999\n",
            "Epoch 3/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4208 - accuracy: 0.7931 - val_loss: 0.4227 - val_accuracy: 0.7890\n",
            "Epoch 4/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4136 - accuracy: 0.7987 - val_loss: 0.4161 - val_accuracy: 0.8010\n",
            "Epoch 5/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4077 - accuracy: 0.8006 - val_loss: 0.4070 - val_accuracy: 0.7987\n",
            "Epoch 6/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4046 - accuracy: 0.8019 - val_loss: 0.4160 - val_accuracy: 0.8039\n",
            "Epoch 7/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4029 - accuracy: 0.8012 - val_loss: 0.4091 - val_accuracy: 0.8062\n",
            "Epoch 8/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.4004 - accuracy: 0.8048 - val_loss: 0.4073 - val_accuracy: 0.8091\n",
            "Epoch 9/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3980 - accuracy: 0.8082 - val_loss: 0.4091 - val_accuracy: 0.8056\n",
            "Epoch 10/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3963 - accuracy: 0.8056 - val_loss: 0.4058 - val_accuracy: 0.8074\n",
            "Epoch 11/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3932 - accuracy: 0.8079 - val_loss: 0.4076 - val_accuracy: 0.8114\n",
            "Epoch 12/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3928 - accuracy: 0.8107 - val_loss: 0.4143 - val_accuracy: 0.8028\n",
            "Epoch 13/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3902 - accuracy: 0.8086 - val_loss: 0.4076 - val_accuracy: 0.8068\n",
            "Epoch 14/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3883 - accuracy: 0.8094 - val_loss: 0.4120 - val_accuracy: 0.8051\n",
            "Epoch 15/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3873 - accuracy: 0.8090 - val_loss: 0.4129 - val_accuracy: 0.8074\n",
            "Epoch 16/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3858 - accuracy: 0.8121 - val_loss: 0.4166 - val_accuracy: 0.7999\n",
            "Epoch 17/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3844 - accuracy: 0.8148 - val_loss: 0.4136 - val_accuracy: 0.8114\n",
            "Epoch 18/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3810 - accuracy: 0.8153 - val_loss: 0.4084 - val_accuracy: 0.8068\n",
            "Epoch 19/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3819 - accuracy: 0.8157 - val_loss: 0.4103 - val_accuracy: 0.8068\n",
            "Epoch 20/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3815 - accuracy: 0.8144 - val_loss: 0.4136 - val_accuracy: 0.8033\n",
            "Epoch 21/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3796 - accuracy: 0.8127 - val_loss: 0.4148 - val_accuracy: 0.8068\n",
            "Epoch 22/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3782 - accuracy: 0.8180 - val_loss: 0.4140 - val_accuracy: 0.8056\n",
            "Epoch 23/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3764 - accuracy: 0.8199 - val_loss: 0.4124 - val_accuracy: 0.8114\n",
            "Epoch 24/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3751 - accuracy: 0.8176 - val_loss: 0.4183 - val_accuracy: 0.8085\n",
            "Epoch 25/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3746 - accuracy: 0.8178 - val_loss: 0.4226 - val_accuracy: 0.7999\n",
            "Epoch 26/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3735 - accuracy: 0.8192 - val_loss: 0.4235 - val_accuracy: 0.7999\n",
            "Epoch 27/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3737 - accuracy: 0.8221 - val_loss: 0.4323 - val_accuracy: 0.7987\n",
            "Epoch 28/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3717 - accuracy: 0.8180 - val_loss: 0.4316 - val_accuracy: 0.7907\n",
            "Epoch 29/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3719 - accuracy: 0.8215 - val_loss: 0.4238 - val_accuracy: 0.8028\n",
            "Epoch 30/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3688 - accuracy: 0.8199 - val_loss: 0.4223 - val_accuracy: 0.8068\n",
            "Epoch 31/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3684 - accuracy: 0.8251 - val_loss: 0.4238 - val_accuracy: 0.8062\n",
            "Epoch 32/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3675 - accuracy: 0.8257 - val_loss: 0.4257 - val_accuracy: 0.8045\n",
            "Epoch 33/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3664 - accuracy: 0.8228 - val_loss: 0.4386 - val_accuracy: 0.7941\n",
            "Epoch 34/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3665 - accuracy: 0.8245 - val_loss: 0.4262 - val_accuracy: 0.8022\n",
            "Epoch 35/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3648 - accuracy: 0.8265 - val_loss: 0.4284 - val_accuracy: 0.8068\n",
            "Epoch 36/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3647 - accuracy: 0.8267 - val_loss: 0.4258 - val_accuracy: 0.8010\n",
            "Epoch 37/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3631 - accuracy: 0.8301 - val_loss: 0.4357 - val_accuracy: 0.7924\n",
            "Epoch 38/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3611 - accuracy: 0.8253 - val_loss: 0.4405 - val_accuracy: 0.7959\n",
            "Epoch 39/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3615 - accuracy: 0.8249 - val_loss: 0.4322 - val_accuracy: 0.7970\n",
            "Epoch 40/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8270 - val_loss: 0.4606 - val_accuracy: 0.7832\n",
            "Epoch 41/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3603 - accuracy: 0.8268 - val_loss: 0.4427 - val_accuracy: 0.7941\n",
            "Epoch 42/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3576 - accuracy: 0.8268 - val_loss: 0.4358 - val_accuracy: 0.7970\n",
            "Epoch 43/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3573 - accuracy: 0.8280 - val_loss: 0.4456 - val_accuracy: 0.7987\n",
            "Epoch 44/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3554 - accuracy: 0.8318 - val_loss: 0.4761 - val_accuracy: 0.7872\n",
            "Epoch 45/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3573 - accuracy: 0.8280 - val_loss: 0.4437 - val_accuracy: 0.7959\n",
            "Epoch 46/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3541 - accuracy: 0.8291 - val_loss: 0.4477 - val_accuracy: 0.7976\n",
            "Epoch 47/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3540 - accuracy: 0.8309 - val_loss: 0.4484 - val_accuracy: 0.8016\n",
            "Epoch 48/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3520 - accuracy: 0.8301 - val_loss: 0.4691 - val_accuracy: 0.7867\n",
            "Epoch 49/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3527 - accuracy: 0.8299 - val_loss: 0.4558 - val_accuracy: 0.7941\n",
            "Epoch 50/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3498 - accuracy: 0.8307 - val_loss: 0.4602 - val_accuracy: 0.7941\n",
            "Epoch 51/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3515 - accuracy: 0.8272 - val_loss: 0.4620 - val_accuracy: 0.7918\n",
            "Epoch 52/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3510 - accuracy: 0.8313 - val_loss: 0.4750 - val_accuracy: 0.7826\n",
            "Epoch 53/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3502 - accuracy: 0.8307 - val_loss: 0.4741 - val_accuracy: 0.7884\n",
            "Epoch 54/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3485 - accuracy: 0.8311 - val_loss: 0.4582 - val_accuracy: 0.7901\n",
            "Epoch 55/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3478 - accuracy: 0.8353 - val_loss: 0.4506 - val_accuracy: 0.7947\n",
            "Epoch 56/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3483 - accuracy: 0.8297 - val_loss: 0.4569 - val_accuracy: 0.7976\n",
            "Epoch 57/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3494 - accuracy: 0.8313 - val_loss: 0.4566 - val_accuracy: 0.7907\n",
            "Epoch 58/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3460 - accuracy: 0.8316 - val_loss: 0.4675 - val_accuracy: 0.7895\n",
            "Epoch 59/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3457 - accuracy: 0.8299 - val_loss: 0.4657 - val_accuracy: 0.8010\n",
            "Epoch 60/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3424 - accuracy: 0.8347 - val_loss: 0.4619 - val_accuracy: 0.7930\n",
            "Epoch 61/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3452 - accuracy: 0.8341 - val_loss: 0.4654 - val_accuracy: 0.7901\n",
            "Epoch 62/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3428 - accuracy: 0.8343 - val_loss: 0.4617 - val_accuracy: 0.7947\n",
            "Epoch 63/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3410 - accuracy: 0.8332 - val_loss: 0.5023 - val_accuracy: 0.7849\n",
            "Epoch 64/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3420 - accuracy: 0.8332 - val_loss: 0.4794 - val_accuracy: 0.7901\n",
            "Epoch 65/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3412 - accuracy: 0.8337 - val_loss: 0.4689 - val_accuracy: 0.7895\n",
            "Epoch 66/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3381 - accuracy: 0.8355 - val_loss: 0.4877 - val_accuracy: 0.7890\n",
            "Epoch 67/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3400 - accuracy: 0.8385 - val_loss: 0.4835 - val_accuracy: 0.7913\n",
            "Epoch 68/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3382 - accuracy: 0.8336 - val_loss: 0.4890 - val_accuracy: 0.7890\n",
            "Epoch 69/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8326 - val_loss: 0.4853 - val_accuracy: 0.7918\n",
            "Epoch 70/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3359 - accuracy: 0.8353 - val_loss: 0.4914 - val_accuracy: 0.7861\n",
            "Epoch 71/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3358 - accuracy: 0.8389 - val_loss: 0.4962 - val_accuracy: 0.7941\n",
            "Epoch 72/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.8395 - val_loss: 0.4893 - val_accuracy: 0.7815\n",
            "Epoch 73/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3337 - accuracy: 0.8393 - val_loss: 0.4939 - val_accuracy: 0.7907\n",
            "Epoch 74/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3321 - accuracy: 0.8364 - val_loss: 0.5186 - val_accuracy: 0.7792\n",
            "Epoch 75/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3345 - accuracy: 0.8372 - val_loss: 0.5053 - val_accuracy: 0.7849\n",
            "Epoch 76/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3337 - accuracy: 0.8351 - val_loss: 0.4991 - val_accuracy: 0.7867\n",
            "Epoch 77/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8360 - val_loss: 0.5060 - val_accuracy: 0.7907\n",
            "Epoch 78/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3330 - accuracy: 0.8384 - val_loss: 0.4901 - val_accuracy: 0.7884\n",
            "Epoch 79/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8330 - val_loss: 0.4998 - val_accuracy: 0.7867\n",
            "Epoch 80/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3294 - accuracy: 0.8395 - val_loss: 0.5072 - val_accuracy: 0.7838\n",
            "Epoch 81/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8385 - val_loss: 0.5091 - val_accuracy: 0.7913\n",
            "Epoch 82/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3300 - accuracy: 0.8368 - val_loss: 0.5160 - val_accuracy: 0.7826\n",
            "Epoch 83/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3269 - accuracy: 0.8366 - val_loss: 0.5176 - val_accuracy: 0.7867\n",
            "Epoch 84/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3292 - accuracy: 0.8428 - val_loss: 0.4978 - val_accuracy: 0.7918\n",
            "Epoch 85/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3243 - accuracy: 0.8410 - val_loss: 0.5225 - val_accuracy: 0.7964\n",
            "Epoch 86/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3262 - accuracy: 0.8418 - val_loss: 0.5198 - val_accuracy: 0.7872\n",
            "Epoch 87/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3235 - accuracy: 0.8426 - val_loss: 0.5302 - val_accuracy: 0.7895\n",
            "Epoch 88/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3230 - accuracy: 0.8382 - val_loss: 0.5340 - val_accuracy: 0.7936\n",
            "Epoch 89/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8412 - val_loss: 0.5326 - val_accuracy: 0.7746\n",
            "Epoch 90/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3240 - accuracy: 0.8408 - val_loss: 0.5142 - val_accuracy: 0.7907\n",
            "Epoch 91/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3216 - accuracy: 0.8435 - val_loss: 0.5304 - val_accuracy: 0.7907\n",
            "Epoch 92/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3207 - accuracy: 0.8431 - val_loss: 0.5542 - val_accuracy: 0.7838\n",
            "Epoch 93/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3205 - accuracy: 0.8435 - val_loss: 0.5375 - val_accuracy: 0.7861\n",
            "Epoch 94/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8405 - val_loss: 0.5548 - val_accuracy: 0.7803\n",
            "Epoch 95/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3172 - accuracy: 0.8422 - val_loss: 0.5360 - val_accuracy: 0.7895\n",
            "Epoch 96/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3193 - accuracy: 0.8403 - val_loss: 0.5611 - val_accuracy: 0.7826\n",
            "Epoch 97/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3195 - accuracy: 0.8408 - val_loss: 0.5442 - val_accuracy: 0.7895\n",
            "Epoch 98/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3170 - accuracy: 0.8430 - val_loss: 0.5378 - val_accuracy: 0.7930\n",
            "Epoch 99/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8426 - val_loss: 0.5474 - val_accuracy: 0.7901\n",
            "Epoch 100/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3168 - accuracy: 0.8437 - val_loss: 0.5393 - val_accuracy: 0.7849\n",
            "Epoch 101/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3158 - accuracy: 0.8443 - val_loss: 0.5549 - val_accuracy: 0.7895\n",
            "Epoch 102/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3124 - accuracy: 0.8468 - val_loss: 0.5900 - val_accuracy: 0.7660\n",
            "Epoch 103/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3120 - accuracy: 0.8458 - val_loss: 0.5604 - val_accuracy: 0.7884\n",
            "Epoch 104/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3137 - accuracy: 0.8458 - val_loss: 0.5573 - val_accuracy: 0.7913\n",
            "Epoch 105/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3144 - accuracy: 0.8466 - val_loss: 0.5573 - val_accuracy: 0.7855\n",
            "Epoch 106/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3133 - accuracy: 0.8454 - val_loss: 0.5606 - val_accuracy: 0.7838\n",
            "Epoch 107/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3102 - accuracy: 0.8481 - val_loss: 0.6251 - val_accuracy: 0.7688\n",
            "Epoch 108/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3102 - accuracy: 0.8431 - val_loss: 0.5879 - val_accuracy: 0.7861\n",
            "Epoch 109/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3134 - accuracy: 0.8454 - val_loss: 0.5895 - val_accuracy: 0.7878\n",
            "Epoch 110/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3133 - accuracy: 0.8458 - val_loss: 0.5785 - val_accuracy: 0.7723\n",
            "Epoch 111/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3104 - accuracy: 0.8477 - val_loss: 0.5752 - val_accuracy: 0.7821\n",
            "Epoch 112/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3110 - accuracy: 0.8458 - val_loss: 0.5821 - val_accuracy: 0.7838\n",
            "Epoch 113/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3093 - accuracy: 0.8499 - val_loss: 0.5845 - val_accuracy: 0.7826\n",
            "Epoch 114/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3090 - accuracy: 0.8470 - val_loss: 0.5901 - val_accuracy: 0.7884\n",
            "Epoch 115/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3053 - accuracy: 0.8477 - val_loss: 0.6354 - val_accuracy: 0.7832\n",
            "Epoch 116/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3094 - accuracy: 0.8468 - val_loss: 0.6247 - val_accuracy: 0.7821\n",
            "Epoch 117/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3083 - accuracy: 0.8454 - val_loss: 0.5877 - val_accuracy: 0.7867\n",
            "Epoch 118/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3065 - accuracy: 0.8487 - val_loss: 0.6285 - val_accuracy: 0.7752\n",
            "Epoch 119/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3069 - accuracy: 0.8508 - val_loss: 0.6161 - val_accuracy: 0.7861\n",
            "Epoch 120/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3041 - accuracy: 0.8504 - val_loss: 0.6555 - val_accuracy: 0.7878\n",
            "Epoch 121/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3065 - accuracy: 0.8510 - val_loss: 0.6256 - val_accuracy: 0.7901\n",
            "Epoch 122/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3067 - accuracy: 0.8497 - val_loss: 0.6258 - val_accuracy: 0.7849\n",
            "Epoch 123/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8481 - val_loss: 0.6452 - val_accuracy: 0.7786\n",
            "Epoch 124/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3040 - accuracy: 0.8504 - val_loss: 0.6235 - val_accuracy: 0.7809\n",
            "Epoch 125/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3026 - accuracy: 0.8504 - val_loss: 0.6435 - val_accuracy: 0.7844\n",
            "Epoch 126/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8529 - val_loss: 0.6175 - val_accuracy: 0.7861\n",
            "Epoch 127/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3043 - accuracy: 0.8535 - val_loss: 0.6372 - val_accuracy: 0.7849\n",
            "Epoch 128/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3045 - accuracy: 0.8506 - val_loss: 0.6378 - val_accuracy: 0.7757\n",
            "Epoch 129/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3010 - accuracy: 0.8556 - val_loss: 0.6431 - val_accuracy: 0.7861\n",
            "Epoch 130/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3010 - accuracy: 0.8502 - val_loss: 0.6553 - val_accuracy: 0.7775\n",
            "Epoch 131/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3020 - accuracy: 0.8504 - val_loss: 0.6711 - val_accuracy: 0.7769\n",
            "Epoch 132/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.3021 - accuracy: 0.8485 - val_loss: 0.6395 - val_accuracy: 0.7861\n",
            "Epoch 133/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2994 - accuracy: 0.8508 - val_loss: 0.6647 - val_accuracy: 0.7890\n",
            "Epoch 134/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2998 - accuracy: 0.8539 - val_loss: 0.6954 - val_accuracy: 0.7792\n",
            "Epoch 135/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2986 - accuracy: 0.8512 - val_loss: 0.7119 - val_accuracy: 0.7832\n",
            "Epoch 136/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8493 - val_loss: 0.6660 - val_accuracy: 0.7867\n",
            "Epoch 137/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2972 - accuracy: 0.8566 - val_loss: 0.6755 - val_accuracy: 0.7826\n",
            "Epoch 138/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2966 - accuracy: 0.8552 - val_loss: 0.7416 - val_accuracy: 0.7688\n",
            "Epoch 139/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.8579 - val_loss: 0.6614 - val_accuracy: 0.7849\n",
            "Epoch 140/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2954 - accuracy: 0.8562 - val_loss: 0.6974 - val_accuracy: 0.7821\n",
            "Epoch 141/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2975 - accuracy: 0.8579 - val_loss: 0.7053 - val_accuracy: 0.7855\n",
            "Epoch 142/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2992 - accuracy: 0.8516 - val_loss: 0.6767 - val_accuracy: 0.7838\n",
            "Epoch 143/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2953 - accuracy: 0.8568 - val_loss: 0.6626 - val_accuracy: 0.7798\n",
            "Epoch 144/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2957 - accuracy: 0.8548 - val_loss: 0.6904 - val_accuracy: 0.7849\n",
            "Epoch 145/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2941 - accuracy: 0.8552 - val_loss: 0.7047 - val_accuracy: 0.7826\n",
            "Epoch 146/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2941 - accuracy: 0.8570 - val_loss: 0.7061 - val_accuracy: 0.7907\n",
            "Epoch 147/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.8575 - val_loss: 0.6878 - val_accuracy: 0.7809\n",
            "Epoch 148/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8568 - val_loss: 0.7359 - val_accuracy: 0.7815\n",
            "Epoch 149/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2945 - accuracy: 0.8533 - val_loss: 0.7371 - val_accuracy: 0.7780\n",
            "Epoch 150/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2924 - accuracy: 0.8589 - val_loss: 0.7230 - val_accuracy: 0.7763\n",
            "Epoch 151/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2924 - accuracy: 0.8564 - val_loss: 0.7475 - val_accuracy: 0.7677\n",
            "Epoch 152/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2907 - accuracy: 0.8581 - val_loss: 0.7531 - val_accuracy: 0.7803\n",
            "Epoch 153/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2952 - accuracy: 0.8550 - val_loss: 0.7277 - val_accuracy: 0.7855\n",
            "Epoch 154/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2892 - accuracy: 0.8543 - val_loss: 0.6955 - val_accuracy: 0.7786\n",
            "Epoch 155/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8547 - val_loss: 0.7304 - val_accuracy: 0.7809\n",
            "Epoch 156/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8556 - val_loss: 0.7602 - val_accuracy: 0.7815\n",
            "Epoch 157/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2917 - accuracy: 0.8556 - val_loss: 0.7421 - val_accuracy: 0.7809\n",
            "Epoch 158/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2907 - accuracy: 0.8558 - val_loss: 0.7832 - val_accuracy: 0.7792\n",
            "Epoch 159/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2907 - accuracy: 0.8537 - val_loss: 0.7472 - val_accuracy: 0.7780\n",
            "Epoch 160/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2922 - accuracy: 0.8558 - val_loss: 0.7900 - val_accuracy: 0.7849\n",
            "Epoch 161/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2910 - accuracy: 0.8604 - val_loss: 0.7945 - val_accuracy: 0.7867\n",
            "Epoch 162/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2915 - accuracy: 0.8594 - val_loss: 0.7496 - val_accuracy: 0.7826\n",
            "Epoch 163/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8568 - val_loss: 0.7503 - val_accuracy: 0.7780\n",
            "Epoch 164/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2889 - accuracy: 0.8583 - val_loss: 0.7434 - val_accuracy: 0.7798\n",
            "Epoch 165/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8604 - val_loss: 0.7514 - val_accuracy: 0.7844\n",
            "Epoch 166/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2858 - accuracy: 0.8581 - val_loss: 0.7880 - val_accuracy: 0.7832\n",
            "Epoch 167/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2842 - accuracy: 0.8598 - val_loss: 0.7926 - val_accuracy: 0.7821\n",
            "Epoch 168/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2870 - accuracy: 0.8556 - val_loss: 0.7794 - val_accuracy: 0.7763\n",
            "Epoch 169/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2885 - accuracy: 0.8610 - val_loss: 0.8295 - val_accuracy: 0.7855\n",
            "Epoch 170/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2839 - accuracy: 0.8610 - val_loss: 0.8000 - val_accuracy: 0.7901\n",
            "Epoch 171/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2850 - accuracy: 0.8606 - val_loss: 0.7870 - val_accuracy: 0.7855\n",
            "Epoch 172/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2864 - accuracy: 0.8625 - val_loss: 0.8016 - val_accuracy: 0.7849\n",
            "Epoch 173/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8577 - val_loss: 0.8085 - val_accuracy: 0.7844\n",
            "Epoch 174/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8602 - val_loss: 0.8118 - val_accuracy: 0.7826\n",
            "Epoch 175/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2824 - accuracy: 0.8589 - val_loss: 0.8444 - val_accuracy: 0.7803\n",
            "Epoch 176/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8614 - val_loss: 0.8526 - val_accuracy: 0.7780\n",
            "Epoch 177/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8616 - val_loss: 0.8615 - val_accuracy: 0.7740\n",
            "Epoch 178/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2845 - accuracy: 0.8581 - val_loss: 0.8029 - val_accuracy: 0.7809\n",
            "Epoch 179/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8625 - val_loss: 0.7968 - val_accuracy: 0.7861\n",
            "Epoch 180/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2822 - accuracy: 0.8602 - val_loss: 0.8170 - val_accuracy: 0.7844\n",
            "Epoch 181/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2830 - accuracy: 0.8637 - val_loss: 0.8157 - val_accuracy: 0.7867\n",
            "Epoch 182/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2809 - accuracy: 0.8631 - val_loss: 0.8367 - val_accuracy: 0.7901\n",
            "Epoch 183/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8637 - val_loss: 0.8687 - val_accuracy: 0.7913\n",
            "Epoch 184/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8629 - val_loss: 0.8755 - val_accuracy: 0.7826\n",
            "Epoch 185/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8617 - val_loss: 0.8552 - val_accuracy: 0.7878\n",
            "Epoch 186/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2796 - accuracy: 0.8639 - val_loss: 0.8917 - val_accuracy: 0.7803\n",
            "Epoch 187/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2793 - accuracy: 0.8612 - val_loss: 0.8720 - val_accuracy: 0.7821\n",
            "Epoch 188/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2791 - accuracy: 0.8625 - val_loss: 0.9733 - val_accuracy: 0.7746\n",
            "Epoch 189/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8629 - val_loss: 0.9013 - val_accuracy: 0.7809\n",
            "Epoch 190/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8598 - val_loss: 0.8956 - val_accuracy: 0.7780\n",
            "Epoch 191/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8662 - val_loss: 0.8889 - val_accuracy: 0.7809\n",
            "Epoch 192/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8621 - val_loss: 0.9042 - val_accuracy: 0.7844\n",
            "Epoch 193/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8629 - val_loss: 0.9347 - val_accuracy: 0.7838\n",
            "Epoch 194/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8646 - val_loss: 0.9271 - val_accuracy: 0.7769\n",
            "Epoch 195/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2718 - accuracy: 0.8677 - val_loss: 0.9401 - val_accuracy: 0.7832\n",
            "Epoch 196/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8652 - val_loss: 0.9335 - val_accuracy: 0.7792\n",
            "Epoch 197/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8698 - val_loss: 0.9564 - val_accuracy: 0.7832\n",
            "Epoch 198/300\n",
            "163/163 [==============================] - 1s 6ms/step - loss: 0.2819 - accuracy: 0.8623 - val_loss: 0.9024 - val_accuracy: 0.7694\n",
            "Epoch 199/300\n",
            "163/163 [==============================] - 1s 6ms/step - loss: 0.2714 - accuracy: 0.8671 - val_loss: 0.9069 - val_accuracy: 0.7867\n",
            "Epoch 200/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2736 - accuracy: 0.8639 - val_loss: 0.9617 - val_accuracy: 0.7815\n",
            "Epoch 201/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8652 - val_loss: 0.9508 - val_accuracy: 0.7838\n",
            "Epoch 202/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.8662 - val_loss: 0.9349 - val_accuracy: 0.7867\n",
            "Epoch 203/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2757 - accuracy: 0.8665 - val_loss: 1.0010 - val_accuracy: 0.7625\n",
            "Epoch 204/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8608 - val_loss: 0.9400 - val_accuracy: 0.7757\n",
            "Epoch 205/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2749 - accuracy: 0.8608 - val_loss: 0.9532 - val_accuracy: 0.7867\n",
            "Epoch 206/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2717 - accuracy: 0.8646 - val_loss: 0.9475 - val_accuracy: 0.7901\n",
            "Epoch 207/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2718 - accuracy: 0.8662 - val_loss: 0.9771 - val_accuracy: 0.7798\n",
            "Epoch 208/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.8650 - val_loss: 0.9745 - val_accuracy: 0.7803\n",
            "Epoch 209/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2703 - accuracy: 0.8606 - val_loss: 1.0079 - val_accuracy: 0.7763\n",
            "Epoch 210/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2715 - accuracy: 0.8665 - val_loss: 0.9885 - val_accuracy: 0.7826\n",
            "Epoch 211/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2718 - accuracy: 0.8665 - val_loss: 1.0383 - val_accuracy: 0.7815\n",
            "Epoch 212/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8637 - val_loss: 1.0467 - val_accuracy: 0.7711\n",
            "Epoch 213/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2722 - accuracy: 0.8660 - val_loss: 0.9727 - val_accuracy: 0.7752\n",
            "Epoch 214/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8658 - val_loss: 0.9827 - val_accuracy: 0.7890\n",
            "Epoch 215/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2685 - accuracy: 0.8635 - val_loss: 1.0869 - val_accuracy: 0.7706\n",
            "Epoch 216/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8700 - val_loss: 1.0047 - val_accuracy: 0.7757\n",
            "Epoch 217/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8702 - val_loss: 0.9972 - val_accuracy: 0.7763\n",
            "Epoch 218/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.8698 - val_loss: 1.0293 - val_accuracy: 0.7757\n",
            "Epoch 219/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8665 - val_loss: 1.0115 - val_accuracy: 0.7798\n",
            "Epoch 220/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8715 - val_loss: 1.0359 - val_accuracy: 0.7746\n",
            "Epoch 221/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8677 - val_loss: 1.1453 - val_accuracy: 0.7775\n",
            "Epoch 222/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8679 - val_loss: 1.1167 - val_accuracy: 0.7821\n",
            "Epoch 223/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8719 - val_loss: 1.0425 - val_accuracy: 0.7729\n",
            "Epoch 224/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2678 - accuracy: 0.8694 - val_loss: 1.1273 - val_accuracy: 0.7803\n",
            "Epoch 225/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2693 - accuracy: 0.8686 - val_loss: 1.0273 - val_accuracy: 0.7711\n",
            "Epoch 226/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8685 - val_loss: 1.0324 - val_accuracy: 0.7780\n",
            "Epoch 227/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2671 - accuracy: 0.8667 - val_loss: 0.9854 - val_accuracy: 0.7826\n",
            "Epoch 228/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2695 - accuracy: 0.8665 - val_loss: 1.1032 - val_accuracy: 0.7821\n",
            "Epoch 229/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8675 - val_loss: 1.0885 - val_accuracy: 0.7821\n",
            "Epoch 230/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2675 - accuracy: 0.8683 - val_loss: 1.0517 - val_accuracy: 0.7809\n",
            "Epoch 231/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.8683 - val_loss: 1.0601 - val_accuracy: 0.7711\n",
            "Epoch 232/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2685 - accuracy: 0.8654 - val_loss: 1.0857 - val_accuracy: 0.7798\n",
            "Epoch 233/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2683 - accuracy: 0.8688 - val_loss: 1.1333 - val_accuracy: 0.7838\n",
            "Epoch 234/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.8677 - val_loss: 1.0909 - val_accuracy: 0.7809\n",
            "Epoch 235/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8702 - val_loss: 1.1234 - val_accuracy: 0.7832\n",
            "Epoch 236/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2696 - accuracy: 0.8669 - val_loss: 1.0528 - val_accuracy: 0.7786\n",
            "Epoch 237/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2662 - accuracy: 0.8734 - val_loss: 1.1028 - val_accuracy: 0.7855\n",
            "Epoch 238/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2640 - accuracy: 0.8688 - val_loss: 1.0883 - val_accuracy: 0.7769\n",
            "Epoch 239/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2642 - accuracy: 0.8685 - val_loss: 1.1378 - val_accuracy: 0.7688\n",
            "Epoch 240/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8656 - val_loss: 1.1662 - val_accuracy: 0.7665\n",
            "Epoch 241/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8648 - val_loss: 1.1525 - val_accuracy: 0.7826\n",
            "Epoch 242/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8673 - val_loss: 1.1951 - val_accuracy: 0.7803\n",
            "Epoch 243/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8733 - val_loss: 1.2109 - val_accuracy: 0.7815\n",
            "Epoch 244/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.8681 - val_loss: 1.1626 - val_accuracy: 0.7786\n",
            "Epoch 245/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2613 - accuracy: 0.8736 - val_loss: 1.1713 - val_accuracy: 0.7740\n",
            "Epoch 246/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.8698 - val_loss: 1.1640 - val_accuracy: 0.7769\n",
            "Epoch 247/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2614 - accuracy: 0.8715 - val_loss: 1.2785 - val_accuracy: 0.7821\n",
            "Epoch 248/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8731 - val_loss: 1.2717 - val_accuracy: 0.7775\n",
            "Epoch 249/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8708 - val_loss: 1.2921 - val_accuracy: 0.7832\n",
            "Epoch 250/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.8729 - val_loss: 1.2073 - val_accuracy: 0.7838\n",
            "Epoch 251/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2590 - accuracy: 0.8688 - val_loss: 1.2064 - val_accuracy: 0.7734\n",
            "Epoch 252/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8686 - val_loss: 1.2110 - val_accuracy: 0.7688\n",
            "Epoch 253/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8706 - val_loss: 1.2589 - val_accuracy: 0.7700\n",
            "Epoch 254/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8696 - val_loss: 1.3063 - val_accuracy: 0.7775\n",
            "Epoch 255/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2614 - accuracy: 0.8673 - val_loss: 1.2360 - val_accuracy: 0.7826\n",
            "Epoch 256/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8734 - val_loss: 1.2678 - val_accuracy: 0.7752\n",
            "Epoch 257/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8756 - val_loss: 1.2901 - val_accuracy: 0.7884\n",
            "Epoch 258/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2596 - accuracy: 0.8713 - val_loss: 1.2209 - val_accuracy: 0.7809\n",
            "Epoch 259/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8729 - val_loss: 1.2586 - val_accuracy: 0.7671\n",
            "Epoch 260/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.8733 - val_loss: 1.2515 - val_accuracy: 0.7786\n",
            "Epoch 261/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2670 - accuracy: 0.8742 - val_loss: 1.2425 - val_accuracy: 0.7792\n",
            "Epoch 262/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8717 - val_loss: 1.1668 - val_accuracy: 0.7740\n",
            "Epoch 263/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2595 - accuracy: 0.8731 - val_loss: 1.2487 - val_accuracy: 0.7763\n",
            "Epoch 264/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2557 - accuracy: 0.8750 - val_loss: 1.2417 - val_accuracy: 0.7809\n",
            "Epoch 265/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2598 - accuracy: 0.8748 - val_loss: 1.3042 - val_accuracy: 0.7694\n",
            "Epoch 266/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2627 - accuracy: 0.8738 - val_loss: 1.3233 - val_accuracy: 0.7786\n",
            "Epoch 267/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8738 - val_loss: 1.2484 - val_accuracy: 0.7838\n",
            "Epoch 268/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8742 - val_loss: 1.3516 - val_accuracy: 0.7798\n",
            "Epoch 269/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8721 - val_loss: 1.2695 - val_accuracy: 0.7798\n",
            "Epoch 270/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2574 - accuracy: 0.8727 - val_loss: 1.2905 - val_accuracy: 0.7769\n",
            "Epoch 271/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.8688 - val_loss: 1.2799 - val_accuracy: 0.7665\n",
            "Epoch 272/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8765 - val_loss: 1.2730 - val_accuracy: 0.7815\n",
            "Epoch 273/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8754 - val_loss: 1.2422 - val_accuracy: 0.7798\n",
            "Epoch 274/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8748 - val_loss: 1.3386 - val_accuracy: 0.7740\n",
            "Epoch 275/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2548 - accuracy: 0.8721 - val_loss: 1.4009 - val_accuracy: 0.7809\n",
            "Epoch 276/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8736 - val_loss: 1.3279 - val_accuracy: 0.7746\n",
            "Epoch 277/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8742 - val_loss: 1.3501 - val_accuracy: 0.7763\n",
            "Epoch 278/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.8744 - val_loss: 1.3357 - val_accuracy: 0.7786\n",
            "Epoch 279/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8742 - val_loss: 1.3059 - val_accuracy: 0.7723\n",
            "Epoch 280/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8736 - val_loss: 1.2176 - val_accuracy: 0.7729\n",
            "Epoch 281/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8736 - val_loss: 1.2824 - val_accuracy: 0.7849\n",
            "Epoch 282/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8744 - val_loss: 1.2040 - val_accuracy: 0.7734\n",
            "Epoch 283/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2542 - accuracy: 0.8736 - val_loss: 1.3570 - val_accuracy: 0.7729\n",
            "Epoch 284/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2542 - accuracy: 0.8748 - val_loss: 1.3507 - val_accuracy: 0.7786\n",
            "Epoch 285/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8706 - val_loss: 1.3616 - val_accuracy: 0.7803\n",
            "Epoch 286/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2563 - accuracy: 0.8729 - val_loss: 1.4662 - val_accuracy: 0.7723\n",
            "Epoch 287/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2579 - accuracy: 0.8752 - val_loss: 1.3614 - val_accuracy: 0.7694\n",
            "Epoch 288/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8736 - val_loss: 1.3341 - val_accuracy: 0.7786\n",
            "Epoch 289/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8719 - val_loss: 1.3975 - val_accuracy: 0.7826\n",
            "Epoch 290/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2621 - accuracy: 0.8748 - val_loss: 1.2772 - val_accuracy: 0.7775\n",
            "Epoch 291/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2616 - accuracy: 0.8742 - val_loss: 1.2155 - val_accuracy: 0.7648\n",
            "Epoch 292/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2531 - accuracy: 0.8777 - val_loss: 1.3779 - val_accuracy: 0.7775\n",
            "Epoch 293/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8796 - val_loss: 1.3423 - val_accuracy: 0.7757\n",
            "Epoch 294/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8757 - val_loss: 1.3709 - val_accuracy: 0.7821\n",
            "Epoch 295/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2525 - accuracy: 0.8754 - val_loss: 1.4136 - val_accuracy: 0.7711\n",
            "Epoch 296/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8738 - val_loss: 1.4061 - val_accuracy: 0.7752\n",
            "Epoch 297/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8771 - val_loss: 1.3840 - val_accuracy: 0.7614\n",
            "Epoch 298/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2530 - accuracy: 0.8738 - val_loss: 1.3483 - val_accuracy: 0.7734\n",
            "Epoch 299/300\n",
            "163/163 [==============================] - 1s 4ms/step - loss: 0.2574 - accuracy: 0.8731 - val_loss: 1.4176 - val_accuracy: 0.7786\n",
            "Epoch 300/300\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.2562 - accuracy: 0.8738 - val_loss: 1.4731 - val_accuracy: 0.7757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history_MLP_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "saYSpvK4NQs2",
        "outputId": "1631a00f-5980-49df-8a1f-aaec157a0092"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP_1 모델이 충분히 학습되지 않고, underfitted 되어있다고 판단하였기 때문에 MLP_2 에서는 hidden layer 수를 4개로 증가시켰으며, training set 에 조금 더 fitting 하기 위하여 epochs 수를 300 으로 늘렸다. 그 결과, train accuracy 는 증가했지만, validation accuracy 는 epoch 수가 증가할 수록 감소하는 overfitting 모습을 보였다. 처음에는 layer 를 깊게 쌓고, epoch 수를 증가시키면 더 높은 validation accuracy 를 보일 줄 알았으나, 결과적으로 MLP_1 모델 보다 더 낮은 validaiton accuracy 를 기록하면서 비효과적인 모델이라는 것을 알 수 있었다.**"
      ],
      "metadata": {
        "id": "asbTlnIhNaUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. KNN \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "n_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,21,22,23,24, 25, 30, 35, 40, 45, 50]\n",
        "score = []\n",
        "\n",
        "# 가장 적합한 k를 찾기.\n",
        "for k in n_neighbors_list:\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  score.append(knn.score(X_val, y_val))\n",
        "\n",
        "plt.plot(n_neighbors_list, score)\n",
        "print(n_neighbors_list[np.argmax(score)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "KRC1waeNBz8_",
        "outputId": "01743964-8b25-4013-9614-ccd90e153fae"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=18)\n",
        "knn.fit(X_train, y_train)\n",
        "print(knn.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6EDFQnSW5v3",
        "outputId": "e03afa18-ebb7-4455-fffd-1a0f3428e43c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7952846463484762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN Classifier 를 사용했으며, 상수 k 를 변화시키면서 가장 최적화된 k 값을 먼저 찾았다. 그 결과 k=18 일 때 즉, 의사결정에 고려할 때 주변 18개의 instance 들을 사용할 때 가장 높은 성능을 기록한다는 것을 알 수 있었다. KNN classifier (k=18) 의 validation accuracy 는 0.795 였다.**"
      ],
      "metadata": {
        "id": "mz1DsibQOxPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Ensemble Model - Bagging 모델 \n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "\n",
        "knn_clf = KNeighborsClassifier(n_neighbors = 18)\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(16,8), activation='relu', \n",
        "                          batch_size=32, learning_rate_init=0.001, max_iter=50)\n",
        "dt_clf = DecisionTreeClassifier(min_impurity_decrease = 0.001)\n",
        "\n",
        "\n",
        "Ensemble_model_hard = VotingClassifier(estimators = [('KNN',knn_clf), ('MLP',mlp_clf), ('DT',dt_clf)], voting='hard')\n",
        "Ensemble_model_soft = VotingClassifier(estimators = [('KNN',knn_clf), ('MLP',mlp_clf), ('DT',dt_clf)], voting='soft')\n",
        "\n",
        "\n",
        "knn_clf.fit(X_train, y_train)\n",
        "mlp_clf.fit(X_train, y_train)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "Ensemble_model_hard.fit(X_train, y_train)\n",
        "Ensemble_model_soft.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print('KNN validation score : ', knn_clf.score(X_val, y_val))\n",
        "print('MLP validation score : ', mlp_clf.score(X_val, y_val))\n",
        "print('DT validtaion score : ', dt_clf.score(X_val, y_val))\n",
        "\n",
        "print('Ensemble hard validation score : ', Ensemble_model_hard.score(X_val, y_val))\n",
        "print('Ensemble soft validation score : ', Ensemble_model_soft.score(X_val, y_val))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VuikQOVgeH9",
        "outputId": "f1bdce0c-09ed-45f1-9dfe-760bc5985389"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN validation score :  0.7952846463484762\n",
            "MLP validation score :  0.8108108108108109\n",
            "DT validtaion score :  0.7912593444508338\n",
            "Ensemble hard validation score :  0.816561242093157\n",
            "Ensemble soft validation score :  0.8113858539390454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**마지막 5번째 모델인 앙상블 모델은 지금까지 위에서 살펴보았던 모델 중, 높은 Validation accuracy 를 보였던 모델들로 구성하였다. 총 3개의 모델로 구성하였는데, 첫번째는 KNN classifier ( k = 18) 두번째는 hidden layer ( h_1 = 16, h_2 = 8 )가 2개인 Multi layer perceptron, 마지막으로 min_impurity_decrease = 0.001 으로, parameter 을 튜닝한 Decision Tree 이다. 앙상블 모델 중, 세 가지의 모델의 결과값을 반영하는 방식을 사용하였는데 다수결을 따르는 'hard' 방식, 확률 값을 계산하는 'soft' 방식에 대한 score 를 함께 살펴보았다. 그 결과 세 가지 모델을 독립적으로 사용했을 때의 validation accuracy 는 (KNN : 0.795, MLP : 0.811, DT : 0.791 ) 보다, 위 세가지의 모델을 하나로 묶어, 앙상블 모델로 구축했을 때 더 높은 validation accuracy ( 'hard' : 0.816, 'soft' : 0.811 ) 를 보인다는 점을 알 수 있었다. 그 중에서도, hard 방식의 정확도가 가장 높았다.** "
      ],
      "metadata": {
        "id": "Cle-wjdHetlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "추가적으로 최종 모델의 accuracy 를 높이기 위하여, 모델에 최적화된 feature 를 산출하기 위하여 feature selection 을 진행하기로 하였다. "
      ],
      "metadata": {
        "id": "DZVCxxuLy8GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Ensemble hard test score : ', Ensemble_model_hard.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g71QjPoAgKh4",
        "outputId": "4f06947e-1c7a-4e2b-9ff0-3ded326aa683"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble hard test score :  0.8125359401955147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "> **5. 결론**\n",
        "\n",
        "이번 텀 프로젝트를 통하여, Decision Tree, Perceptron, KNN, MLP, Ensemble 모델에 대한 각각의 validation score 를 살펴보았다. \n",
        "1. Decision Tree : min_impurity_decrease = 0.001 으로 튜닝한 Decision Tree 가 Decision Tree 중 가장 높은 validation score (0.792) 를 보였고, Feature selection 을 통해 24개의 feature 중 17개의 feature 를 사용할 때 성능이 더 증가하는 경향도 확인할 수 있었다. \n",
        "\n",
        "2. Perceptron : 퍼셉트론은 iteration 을 아무리 늘려도, validation score (0.719) 가 이 이상으로 증가하지 않는다는 것을 발견하였고, 이처럼 underfitted 되는 이유는 모델 구조가 단순하기 때문에 복잡한 데이터를 학습하기에는 빈약한 구조라는 점을 알게 되었다. \n",
        "\n",
        "3. KNN : KNN 은 가장 최적화된 k = 18 일 때, validation score (0.795) 로 가장 높은 수치가 나왔다. \n",
        "\n",
        "4. MLP : MLP 는 hiddenn layer 가 2개 일때 가장 높은 validation score (0.811) 를 도출했다. 단일 모델 중에서는 가장 높은 수치였으며, 단순히 layer 를 깊게하고, epochs 를 늘리는 것이 성능을 높이는 방법이 아니라는 것을 알 수 있었다. 모델 구조가 과하게 복잡해지고, epochs 가 증가하면서 overfitting 현상이 발생하였다. \n",
        "\n",
        "5. Ensemble model : 이전에 단일 모델로 높은 성능을 보인, KNN classifier ( k = 18), hidden layer ( h_1 = 16, h_2 = 8 )가 2개인 Multi layer perceptron, 그리고 min_impurity_decrease = 0.001 인 Decision Tree 총 3가지의 모델로 구성되었으며, 3개 아웃풋 다수결을 따르는 hard 방식의 앙상블 모델이다. validation score (0.816) 으로 가장 높았다. \n",
        "\n",
        "> 결국 최종적으로 앙상블 모델을 선택하였고, test accuracy 는 0.813 이였다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IE8s5JGHpZnx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WM8QaVQfk-2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}